import type { Question } from "../types";

export const questionsMlAdvanced: Question[] = [
  // ===== 教師あり学習基礎 =====
  {
    id: "ml-051",
    category: "データサイエンス力",
    question: "教師あり学習において、予測したい対象の変数を何と呼ぶか？",
    choices: [
      "説明変数",
      "目的変数",
      "潜在変数",
      "制御変数",
    ],
    correctIndex: 1,
    explanation:
      "教師あり学習では、予測したい対象の変数を「目的変数（被説明変数・従属変数）」と呼びます。一方、予測のために使用する入力データの変数を「説明変数（独立変数・特徴量）」と呼びます。例えば住宅価格を予測する場合、価格が目的変数、面積や築年数などが説明変数です。",
  },
  {
    id: "ml-052",
    category: "データサイエンス力",
    question: "回帰問題と分類問題の違いとして正しいものはどれか？",
    choices: [
      "回帰は連続値を予測し、分類はカテゴリ（離散値）を予測する",
      "回帰はカテゴリを予測し、分類は連続値を予測する",
      "回帰は教師なし学習で、分類は教師あり学習である",
      "回帰は画像データ専用で、分類はテキストデータ専用である",
    ],
    correctIndex: 0,
    explanation:
      "回帰問題は売上金額や気温のような連続的な数値を予測するタスクです。分類問題はメールがスパムか否か、画像に写っている動物の種類など、離散的なカテゴリを予測するタスクです。どちらも教師あり学習に分類されます。",
  },
  {
    id: "ml-053",
    category: "データサイエンス力",
    question: "線形回帰モデルの説明として正しいものはどれか？",
    choices: [
      "データを複数のクラスタに分割する手法である",
      "説明変数と目的変数の関係を直線（一次式）で表現するモデルである",
      "データをツリー構造で分割して予測するモデルである",
      "確率分布を使ってカテゴリを予測するモデルである",
    ],
    correctIndex: 1,
    explanation:
      "線形回帰は、目的変数 y を説明変数 x の一次式（y = ax + b）で表現するモデルです。複数の説明変数がある場合は重回帰と呼ばれます。モデルがシンプルで解釈しやすいことが利点ですが、非線形な関係は捉えにくいという限界があります。",
  },
  {
    id: "ml-054",
    category: "データサイエンス力",
    question: "ロジスティック回帰の説明として正しいものはどれか？",
    choices: [
      "連続値の予測に使用される回帰手法である",
      "名前に「回帰」とあるが、分類問題に使用される手法である",
      "教師なし学習の手法である",
      "決定木の一種である",
    ],
    correctIndex: 1,
    explanation:
      "ロジスティック回帰は、名前に「回帰」と付いていますが、実際には分類問題に用いられる手法です。シグモイド関数を使って出力を0から1の確率値に変換し、例えば0.5以上なら陽性、未満なら陰性と判定します。二値分類の基本的な手法です。",
  },
  {
    id: "ml-055",
    category: "データサイエンス力",
    question: "教師あり学習の具体例として正しいものはどれか？",
    choices: [
      "顧客を購買傾向に基づいてグループに分ける",
      "過去の販売データから来月の売上を予測する",
      "大量の文書からトピックを自動的に発見する",
      "データの次元を削減して可視化する",
    ],
    correctIndex: 1,
    explanation:
      "教師あり学習は、入力データと正解ラベル（目的変数）のペアを使って学習する手法です。過去の販売データ（入力）と実際の売上（正解）のペアから学習し、未来の売上を予測することが該当します。グループ分けやトピック発見は教師なし学習の例です。",
  },
  // ===== 教師なし学習基礎 =====
  {
    id: "ml-056",
    category: "データサイエンス力",
    question: "クラスタリングの目的として最も適切なものはどれか？",
    choices: [
      "データに正解ラベルを付与する",
      "データを類似度に基づいてグループに分ける",
      "連続値の予測を行う",
      "データの因果関係を明らかにする",
    ],
    correctIndex: 1,
    explanation:
      "クラスタリングは教師なし学習の代表的な手法で、正解ラベルなしにデータの類似度に基づいて自動的にグループ（クラスタ）に分割します。顧客セグメンテーションや文書の自動分類などに活用されます。",
  },
  {
    id: "ml-057",
    category: "データサイエンス力",
    question: "k-meansクラスタリングの基本手順として正しいものはどれか？",
    choices: [
      "データを木構造で階層的に分割していく",
      "クラスタ中心をランダムに配置し、データの割り当てと中心の更新を繰り返す",
      "データの密度が高い領域を探索してクラスタを形成する",
      "データ間の距離行列を計算し、最短距離のペアを結合していく",
    ],
    correctIndex: 1,
    explanation:
      "k-meansは、(1) k個のクラスタ中心をランダムに初期配置、(2) 各データを最も近いクラスタ中心に割り当て、(3) 各クラスタの重心を新しいクラスタ中心として更新、(2)と(3)を収束するまで繰り返す、という手順で動作します。",
  },
  {
    id: "ml-058",
    category: "データサイエンス力",
    question: "主成分分析（PCA）の目的として正しいものはどれか？",
    choices: [
      "データをクラスタに分割する",
      "データの分散が最大となる方向を見つけて次元を削減する",
      "データの欠損値を補完する",
      "データのクラス分類を行う",
    ],
    correctIndex: 1,
    explanation:
      "主成分分析（PCA）は、高次元データの分散が最も大きい方向（主成分）を順に見つけ出し、情報の損失をできるだけ抑えながら次元を削減する手法です。データの可視化や、多くの特徴量を少数の主成分にまとめる際に使用されます。",
  },
  {
    id: "ml-059",
    category: "データサイエンス力",
    question: "教師なし学習に該当しないものはどれか？",
    choices: [
      "k-meansクラスタリング",
      "主成分分析（PCA）",
      "ロジスティック回帰",
      "階層的クラスタリング",
    ],
    correctIndex: 2,
    explanation:
      "ロジスティック回帰は正解ラベルを使って学習する教師あり学習の手法です。k-means、PCA、階層的クラスタリングはいずれも正解ラベルなしで学習する教師なし学習の手法です。",
  },
  {
    id: "ml-060",
    category: "データサイエンス力",
    question: "k-meansクラスタリングの注意点として正しいものはどれか？",
    choices: [
      "クラスタ数kを事前に指定する必要がある",
      "kの値は自動的に最適値が決まる",
      "カテゴリデータに直接適用できる",
      "常に最適なクラスタリング結果が得られる",
    ],
    correctIndex: 0,
    explanation:
      "k-meansではクラスタ数kを事前に指定する必要があります。最適なkの値を決める方法としてエルボー法やシルエット分析があります。また、初期値によって結果が変わる可能性があり、必ずしも最適解が得られるとは限りません。",
  },
  // ===== 決定木・k-NN =====
  {
    id: "ml-061",
    category: "データサイエンス力",
    question: "決定木の仕組みとして正しいものはどれか？",
    choices: [
      "データ間の距離を計算して最も近いデータのラベルを参照する",
      "特徴量に基づいてデータを条件分岐で繰り返し分割し、予測を行う",
      "データの確率分布を推定してカテゴリを予測する",
      "重みを持つ結合で層をなすネットワークで予測する",
    ],
    correctIndex: 1,
    explanation:
      "決定木は「年齢が30以上か？」「収入が500万以上か？」のような条件で、データをツリー（木）構造で繰り返し分割していき、最終的な葉ノードで予測を行うモデルです。結果が人間にとって解釈しやすいことが大きな利点です。",
  },
  {
    id: "ml-062",
    category: "データサイエンス力",
    question: "ランダムフォレストの説明として正しいものはどれか？",
    choices: [
      "1つの決定木を非常に深く成長させる手法",
      "複数の決定木を組み合わせて、多数決や平均で予測するアンサンブル手法",
      "決定木の代わりに線形回帰を使う手法",
      "データを事前にクラスタリングしてから分類する手法",
    ],
    correctIndex: 1,
    explanation:
      "ランダムフォレストは、データのランダムサンプリングと特徴量のランダム選択により多数の決定木を構築し、それらの予測を多数決（分類）や平均（回帰）で統合するアンサンブル学習手法です。単一の決定木より過学習しにくく、高い予測性能を持ちます。",
  },
  {
    id: "ml-063",
    category: "データサイエンス力",
    question: "k-NN（k近傍法）の仕組みとして正しいものはどれか？",
    choices: [
      "データを木構造で分割して分類する",
      "新しいデータに最も近いk個の既知データのラベルから多数決で分類する",
      "データの確率分布を推定して分類する",
      "ニューラルネットワークでデータを分類する",
    ],
    correctIndex: 1,
    explanation:
      "k-NN（k-Nearest Neighbors）は、予測対象のデータに距離が近いk個の学習データを見つけ、それらのラベルの多数決で分類を行うシンプルな手法です。学習時にモデルを構築せず、予測時にすべての学習データとの距離を計算するため「怠惰学習」とも呼ばれます。",
  },
  {
    id: "ml-064",
    category: "データサイエンス力",
    question: "決定木の利点として正しいものはどれか？",
    choices: [
      "常に最も高い予測精度が得られる",
      "モデルの判断根拠が可視化でき、解釈しやすい",
      "大規模データに対して常に高速に動作する",
      "過学習が起こらない",
    ],
    correctIndex: 1,
    explanation:
      "決定木の最大の利点は、分岐条件がそのまま判断根拠として可視化・説明できる点です。「なぜその予測になったか」が理解しやすいため、ビジネスでの意思決定に利用しやすいモデルです。ただし、深い木は過学習しやすいという欠点があります。",
  },
  {
    id: "ml-065",
    category: "データサイエンス力",
    question: "k-NNにおけるkの値を大きくした場合の影響として正しいものはどれか？",
    choices: [
      "モデルがより複雑になり、過学習しやすくなる",
      "決定境界がより滑らかになり、ノイズの影響を受けにくくなる",
      "計算時間が短縮される",
      "必ず予測精度が向上する",
    ],
    correctIndex: 1,
    explanation:
      "kの値を大きくすると、より多くの近傍データを参照するため決定境界が滑らかになり、ノイズに強くなります（バイアスが増加しバリアンスが減少）。逆にkが小さいとノイズに敏感になりますが、複雑なパターンを捉えやすくなります。",
  },
  // ===== モデル評価基礎 =====
  {
    id: "ml-066",
    category: "データサイエンス力",
    question: "訓練データとテストデータを分割する主な理由として正しいものはどれか？",
    choices: [
      "学習時間を短縮するため",
      "未知のデータに対するモデルの汎化性能を正しく評価するため",
      "データの量を2倍に増やすため",
      "異なるアルゴリズムで学習するため",
    ],
    correctIndex: 1,
    explanation:
      "訓練データとテストデータを分けるのは、モデルが学習に使っていない未知のデータに対してどの程度正しく予測できるか（汎化性能）を評価するためです。訓練データだけで評価すると、過学習したモデルでも高い成績になってしまい、実運用での性能を正しく把握できません。",
  },
  {
    id: "ml-067",
    category: "データサイエンス力",
    question: "過学習（オーバーフィッティング）の説明として正しいものはどれか？",
    choices: [
      "訓練データに対しても精度が低い状態",
      "訓練データに対して精度が高いが、未知のデータに対して精度が低い状態",
      "モデルの学習が不十分な状態",
      "データが少なすぎて学習できない状態",
    ],
    correctIndex: 1,
    explanation:
      "過学習とは、モデルが訓練データのパターンだけでなくノイズまで学習してしまい、訓練データには高い精度を示すが、未知のデータ（テストデータ）には精度が低くなる状態です。モデルが複雑すぎる場合やデータが少ない場合に起こりやすくなります。",
  },
  {
    id: "ml-068",
    category: "データサイエンス力",
    question: "適合率（Precision）の定義として正しいものはどれか？",
    choices: [
      "全データのうち正しく予測できた割合",
      "陽性と予測したもののうち、実際に陽性だった割合",
      "実際に陽性のもののうち、正しく陽性と予測できた割合",
      "陰性と予測したもののうち、実際に陰性だった割合",
    ],
    correctIndex: 1,
    explanation:
      "適合率（Precision）は「モデルが陽性と予測したもののうち、実際に陽性だった割合」です。つまり、予測の「正確さ」を表します。例えばスパム判定で、スパムと判定したメールのうち本当にスパムだった割合が適合率です。",
  },
  {
    id: "ml-069",
    category: "データサイエンス力",
    question: "再現率（Recall）の定義として正しいものはどれか？",
    choices: [
      "全データのうち正しく予測できた割合",
      "陽性と予測したもののうち、実際に陽性だった割合",
      "実際に陽性のもののうち、正しく陽性と予測できた割合",
      "適合率と正解率の調和平均",
    ],
    correctIndex: 2,
    explanation:
      "再現率（Recall・感度）は「実際に陽性のもののうち、モデルが正しく陽性と予測できた割合」です。つまり、取りこぼしの少なさを表します。例えば病気の検診で、実際に病気の人をどれだけ見逃さずに検出できたかが再現率です。",
  },
  {
    id: "ml-070",
    category: "データサイエンス力",
    question: "F1値（F1スコア）の説明として正しいものはどれか？",
    choices: [
      "正解率（Accuracy）と同じ指標である",
      "適合率と再現率の調和平均であり、両者のバランスを表す指標である",
      "訓練データに対する精度を表す指標である",
      "モデルの学習速度を表す指標である",
    ],
    correctIndex: 1,
    explanation:
      "F1値は適合率（Precision）と再現率（Recall）の調和平均で、F1 = 2 × (Precision × Recall) / (Precision + Recall) で計算されます。適合率と再現率はトレードオフの関係にあるため、両者のバランスを1つの数値で評価したい場合にF1値が使われます。",
  },
  // ===== ニューラルネットワーク基礎 =====
  {
    id: "ml-071",
    category: "データサイエンス力",
    question: "パーセプトロンの説明として正しいものはどれか？",
    choices: [
      "複数の隠れ層を持つ深いネットワーク構造",
      "入力に重みを掛けて合計し、閾値で0か1を出力するニューラルネットワークの基本単位",
      "画像認識に特化したネットワーク構造",
      "時系列データの処理に特化したネットワーク構造",
    ],
    correctIndex: 1,
    explanation:
      "パーセプトロンはニューラルネットワークの最も基本的な構成要素で、複数の入力にそれぞれ重みを掛けて合計し、その値が閾値を超えるかどうかで出力を決定します。単純パーセプトロンは線形分離可能な問題しか解けませんが、多層にすることで複雑な問題を扱えます。",
  },
  {
    id: "ml-072",
    category: "データサイエンス力",
    question: "ニューラルネットワークの活性化関数の役割として正しいものはどれか？",
    choices: [
      "入力データを正規化する",
      "非線形性を導入し、複雑なパターンの学習を可能にする",
      "学習率を自動で調整する",
      "損失関数の値を計算する",
    ],
    correctIndex: 1,
    explanation:
      "活性化関数はニューラルネットワークに非線形性を導入する役割があります。活性化関数がないと、何層重ねても全体として線形変換にしかならず、複雑な非線形パターンを学習できません。代表的な活性化関数にはReLU、sigmoid、tanhなどがあります。",
  },
  {
    id: "ml-073",
    category: "データサイエンス力",
    question: "ReLU（Rectified Linear Unit）関数の特徴として正しいものはどれか？",
    choices: [
      "出力値が常に0から1の範囲になる",
      "入力が0以下のとき0を、0より大きいときその値をそのまま出力する",
      "入力値を-1から1の範囲に変換する",
      "出力が常に正の値になる",
    ],
    correctIndex: 1,
    explanation:
      "ReLUはf(x) = max(0, x) で定義され、入力が負のとき0、正のときそのままの値を出力するシンプルな活性化関数です。sigmoid関数と比べて勾配消失問題が起きにくく、計算も高速なため、深層学習で最も広く使われる活性化関数の一つです。",
  },
  {
    id: "ml-074",
    category: "データサイエンス力",
    question: "ニューラルネットワークにおける「隠れ層」の役割として正しいものはどれか？",
    choices: [
      "入力データを受け取る層",
      "最終的な予測結果を出力する層",
      "入力から出力への間にある中間処理層で、データの特徴を抽出する",
      "損失関数を計算する層",
    ],
    correctIndex: 2,
    explanation:
      "隠れ層は入力層と出力層の間にある中間的な層で、データの複雑な特徴やパターンを段階的に抽出します。隠れ層の数やニューロン数を増やすとモデルの表現力が上がりますが、過学習のリスクも増えます。隠れ層が多いネットワークを「深層」ニューラルネットワーク（Deep Neural Network）と呼びます。",
  },
  {
    id: "ml-075",
    category: "データサイエンス力",
    question: "勾配降下法の基本的な考え方として正しいものはどれか？",
    choices: [
      "損失関数の値が最も大きくなる方向にパラメータを更新する",
      "損失関数の勾配（傾き）を計算し、損失が小さくなる方向にパラメータを更新する",
      "パラメータをランダムに変更して最良の組み合わせを探す",
      "すべてのパラメータを一度に最適値に設定する",
    ],
    correctIndex: 1,
    explanation:
      "勾配降下法は、損失関数の勾配（各パラメータに対する偏微分）を計算し、損失が減少する方向にパラメータを少しずつ更新していく最適化手法です。学習率というハイパーパラメータで更新の大きさを制御します。ニューラルネットワークの学習の基礎となる手法です。",
  },
  // ===== CNN・画像認識基礎 =====
  {
    id: "ml-076",
    category: "データサイエンス力",
    question: "CNN（畳み込みニューラルネットワーク）の畳み込み層の役割として正しいものはどれか？",
    choices: [
      "画像のサイズを縮小する",
      "フィルタ（カーネル）を用いて画像からエッジや模様などの特徴を抽出する",
      "分類結果を確率として出力する",
      "画像の色を正規化する",
    ],
    correctIndex: 1,
    explanation:
      "畳み込み層は、小さなフィルタ（カーネル）を画像上でスライドさせながら積和演算を行い、エッジ、テクスチャ、模様などの局所的な特徴を抽出します。浅い層では単純なエッジを、深い層ではより複雑なパターンを検出します。",
  },
  {
    id: "ml-077",
    category: "データサイエンス力",
    question: "CNNにおけるプーリング層の役割として正しいものはどれか？",
    choices: [
      "画像から特徴を抽出する",
      "特徴マップのサイズを縮小し、計算量の削減と位置ずれへの頑健性を得る",
      "最終的な分類結果を出力する",
      "入力画像にノイズを加える",
    ],
    correctIndex: 1,
    explanation:
      "プーリング層は特徴マップの小さな領域から最大値（最大プーリング）や平均値（平均プーリング）を取ることで、サイズを縮小します。これにより計算量が減るとともに、対象の位置が多少ずれても同じ特徴が検出されやすくなる（位置不変性）効果があります。",
  },
  {
    id: "ml-078",
    category: "データサイエンス力",
    question: "デジタル画像の構造に関する説明として正しいものはどれか？",
    choices: [
      "カラー画像は1チャネルのピクセル値で構成される",
      "カラー画像は通常RGB（赤・緑・青）の3チャネルのピクセル値で構成される",
      "画像の解像度はチャネル数で決まる",
      "グレースケール画像は3チャネルで構成される",
    ],
    correctIndex: 1,
    explanation:
      "デジタルのカラー画像は、通常RGB（赤・緑・青）の3つのチャネルで構成され、各ピクセルは0〜255の値を持ちます。グレースケール画像は1チャネルです。CNNに入力する際は、このピクセル値の配列をそのまま扱います。",
  },
  {
    id: "ml-079",
    category: "データサイエンス力",
    question: "物体検出（Object Detection）の説明として正しいものはどれか？",
    choices: [
      "画像全体を1つのカテゴリに分類するタスク",
      "画像内の物体の位置をバウンディングボックスで示し、種類を認識するタスク",
      "画像のピクセルごとにカテゴリを割り当てるタスク",
      "画像をテキストで説明するタスク",
    ],
    correctIndex: 1,
    explanation:
      "物体検出は、画像内に存在する物体の位置を矩形（バウンディングボックス）で囲み、同時にその物体のカテゴリ（人、車、犬など）を認識するタスクです。画像分類が画像全体のカテゴリを判定するのに対し、物体検出は「どこに」「何が」あるかを同時に特定します。",
  },
  {
    id: "ml-080",
    category: "データサイエンス力",
    question: "CNNが画像認識で広く使われる理由として最も適切なものはどれか？",
    choices: [
      "テキストデータにも同様に適用できるため",
      "画像の局所的な特徴を効率よく捉え、パラメータ数を抑えられるため",
      "全結合層だけのネットワークより常に高速に動作するため",
      "教師なし学習でも使用できるため",
    ],
    correctIndex: 1,
    explanation:
      "CNNは畳み込みにより画像の局所的な特徴（エッジや模様）を効率的に捉えられます。また、重み共有（同じフィルタを画像全体で使い回す）により、全結合層に比べてパラメータ数を大幅に抑えることができます。これが画像認識でCNNが標準的に使われる理由です。",
  },
  // ===== RNN・自然言語処理基礎 =====
  {
    id: "ml-081",
    category: "データサイエンス力",
    question: "RNN（リカレントニューラルネットワーク）の特徴として正しいものはどれか？",
    choices: [
      "画像の空間的な特徴を捉えるのに適している",
      "過去の情報を内部状態として保持し、系列データ（時系列やテキスト）を処理できる",
      "クラスタリングに特化したネットワークである",
      "入力データの次元削減を行うネットワークである",
    ],
    correctIndex: 1,
    explanation:
      "RNNは前の時刻の出力（隠れ状態）を次の時刻の入力に渡す再帰構造を持ち、過去の情報を記憶しながら系列データを順に処理できます。テキスト、音声、株価などの時系列データの処理に適しています。",
  },
  {
    id: "ml-082",
    category: "データサイエンス力",
    question: "形態素解析の説明として正しいものはどれか？",
    choices: [
      "テキストの感情（ポジティブ/ネガティブ）を判定する処理",
      "日本語などのテキストを意味を持つ最小単位（形態素）に分割する処理",
      "テキストを他の言語に翻訳する処理",
      "テキストの文法的な誤りを検出する処理",
    ],
    correctIndex: 1,
    explanation:
      "形態素解析は、文を形態素（意味を持つ最小の言語単位）に分割し、品詞を付与する処理です。例えば「今日は天気がいい」を「今日/は/天気/が/いい」に分割します。英語は空白で単語が区切られますが、日本語にはスペースがないため形態素解析が必要です。MeCabやJanomeなどのツールが使われます。",
  },
  {
    id: "ml-083",
    category: "データサイエンス力",
    question: "自然言語処理におけるストップワード除去の目的として正しいものはどれか？",
    choices: [
      "テキストの誤字脱字を修正する",
      "「は」「の」「です」のような出現頻度が高く情報量の少ない語を除去し、分析の精度を高める",
      "テキストをすべて小文字に変換する",
      "テキスト中の数値を除去する",
    ],
    correctIndex: 1,
    explanation:
      "ストップワードとは「は」「の」「です」（英語では the, is, a など）のように、出現頻度は高いが文書の特徴を表す上では重要でない語のことです。これらを除去することで、テキストの本質的な内容に関わる語に焦点を当てた分析が可能になります。",
  },
  {
    id: "ml-084",
    category: "データサイエンス力",
    question: "TF-IDFの説明として正しいものはどれか？",
    choices: [
      "単語をベクトル空間に埋め込む深層学習手法",
      "単語の文書内での重要度を「出現頻度」と「希少性」の両面から評価する手法",
      "文書同士の類似度を直接計算する手法",
      "テキストから固有名詞だけを抽出する手法",
    ],
    correctIndex: 1,
    explanation:
      "TF-IDFは、TF（Term Frequency：文書中での単語の出現頻度）とIDF（Inverse Document Frequency：全文書中での単語の希少性）を掛け合わせた指標です。特定の文書に頻出し、かつ他の文書にはあまり出現しない単語ほど高い値をとり、その文書を特徴づける重要な単語を見つけるのに使われます。",
  },
  {
    id: "ml-085",
    category: "データサイエンス力",
    question: "文書分類の具体例として最も適切なものはどれか？",
    choices: [
      "画像に写っている物体を特定する",
      "メールをスパムか正常かに自動判定する",
      "音声をテキストに変換する",
      "商品の売上金額を予測する",
    ],
    correctIndex: 1,
    explanation:
      "文書分類はテキストデータを所定のカテゴリに分類するタスクです。スパムメールの判定、ニュース記事のジャンル分類、商品レビューのポジティブ/ネガティブ判定などが代表的な例です。自然言語処理の基本的な応用の一つです。",
  },
  // ===== 生成AI・LLM基礎 =====
  {
    id: "ml-086",
    category: "データサイエンス力",
    question: "大規模言語モデル（LLM）の説明として正しいものはどれか？",
    choices: [
      "画像認識に特化した小規模なモデル",
      "大量のテキストデータで事前学習された、文章の生成や理解ができる大規模なニューラルネットワークモデル",
      "数値データの回帰分析に特化したモデル",
      "音声認識専用のモデル",
    ],
    correctIndex: 1,
    explanation:
      "大規模言語モデル（LLM: Large Language Model）は、インターネット上の大量のテキストデータを用いて事前学習された、数十億〜数千億のパラメータを持つニューラルネットワークです。GPTやClaude、Geminiなどが代表的で、文章生成、要約、翻訳、質問応答など幅広いタスクに対応できます。",
  },
  {
    id: "ml-087",
    category: "データサイエンス力",
    question: "Transformerアーキテクチャにおける注意機構（Attention）の役割として正しいものはどれか？",
    choices: [
      "入力データの次元を削減する",
      "入力系列中の各要素が他の要素とどの程度関連するかの重みを計算し、重要な情報に注目する",
      "データを教師あり学習と教師なし学習に分ける",
      "モデルのパラメータ数を削減する",
    ],
    correctIndex: 1,
    explanation:
      "注意機構（Attention）は、入力系列のすべての要素間の関連度を計算し、関連性の高い情報に重みを置いて処理する仕組みです。例えば翻訳時に、出力する単語に対して入力文のどの部分が重要かを動的に判断します。TransformerではSelf-Attentionにより、RNNなしで長距離の依存関係を効率的に捉えられます。",
  },
  {
    id: "ml-088",
    category: "データサイエンス力",
    question: "プロンプトエンジニアリングにおけるFew-shotプロンプトの説明として正しいものはどれか？",
    choices: [
      "モデルのパラメータを少量のデータで再学習させる手法",
      "タスクの例を数個プロンプトに含めることで、モデルに期待する出力形式やタスク内容を示す手法",
      "モデルの出力トークン数を制限する手法",
      "プロンプトを複数の言語で記述する手法",
    ],
    correctIndex: 1,
    explanation:
      "Few-shotプロンプトは、LLMへの入力（プロンプト）に望ましい入出力の例を数個含めることで、モデルにタスクの内容と期待する出力形式を理解させるテクニックです。例を含めないZero-shot、1つだけ含めるOne-shotもあります。モデルのパラメータは変更しません。",
  },
  {
    id: "ml-089",
    category: "データサイエンス力",
    question: "生成AIにおける「ハルシネーション（幻覚）」の説明として正しいものはどれか？",
    choices: [
      "AIが画像に幻想的なエフェクトを加えること",
      "AIがもっともらしいが事実と異なる情報を生成してしまう現象",
      "AIが入力を受け付けなくなる現象",
      "AIの計算速度が急激に低下する現象",
    ],
    correctIndex: 1,
    explanation:
      "ハルシネーションとは、LLMなどの生成AIが事実に基づかない、しかしもっともらしい文章を生成してしまう現象です。存在しない文献の引用や、誤った事実の記述などが例です。生成AIの重大な課題の一つであり、出力内容の事実確認（ファクトチェック）が重要とされています。",
  },
  {
    id: "ml-090",
    category: "データサイエンス力",
    question: "ファインチューニングの説明として正しいものはどれか？",
    choices: [
      "モデルをゼロから新しいデータで学習させること",
      "事前学習済みモデルを特定のタスクのデータで追加学習し、性能を調整すること",
      "モデルの推論速度を最適化すること",
      "ハイパーパラメータを手動で調整すること",
    ],
    correctIndex: 1,
    explanation:
      "ファインチューニングは、大規模データで事前学習されたモデルに対して、特定のタスクや分野のデータで追加的に学習を行い、そのタスクでの性能を向上させる手法です。ゼロから学習するよりも少ないデータと計算量で高い性能を達成できるのが利点です。",
  },
  // ===== データ前処理 =====
  {
    id: "ml-091",
    category: "データサイエンス力",
    question: "欠損値の処理方法として適切でないものはどれか？",
    choices: [
      "欠損値を含む行を削除する",
      "平均値や中央値で補完する",
      "欠損値をすべて0で置き換える（常に適切とは限らない）",
      "欠損値があることをそのまま放置して学習に使う（多くのアルゴリズムではエラーになる）",
    ],
    correctIndex: 3,
    explanation:
      "多くの機械学習アルゴリズムは欠損値を含むデータを直接扱えないため、事前に処理が必要です。代表的な方法は、欠損値を含む行の削除、平均値・中央値・最頻値による補完、前後の値による補間などです。そのまま放置するとエラーや誤った結果の原因になります。",
  },
  {
    id: "ml-092",
    category: "データサイエンス力",
    question: "ワンホットエンコーディングの説明として正しいものはどれか？",
    choices: [
      "カテゴリ変数を0と1の順序付き数値に変換する手法",
      "カテゴリの各値に対応するバイナリ（0/1）の列を作成し、該当する列のみ1にする手法",
      "数値データを0から1の範囲に変換する手法",
      "テキストデータを数値に変換する手法",
    ],
    correctIndex: 1,
    explanation:
      "ワンホットエンコーディングは、カテゴリ変数（例：色 = {赤, 青, 緑}）を、カテゴリの数だけ列を作り、該当するカテゴリの列のみ1、他を0とするバイナリ表現に変換する手法です。カテゴリに順序がない場合に適しています。例えば「赤」は [1, 0, 0]、「青」は [0, 1, 0] となります。",
  },
  {
    id: "ml-093",
    category: "データサイエンス力",
    question: "ラベルエンコーディングの説明として正しいものはどれか？",
    choices: [
      "カテゴリ変数を複数のバイナリ列に展開する手法",
      "カテゴリ変数の各値に整数を割り当てる手法（例：赤=0, 青=1, 緑=2）",
      "数値データをカテゴリデータに変換する手法",
      "連続値を離散値に分割する手法",
    ],
    correctIndex: 1,
    explanation:
      "ラベルエンコーディングは、カテゴリ変数の各値に整数を割り当てる手法です（例：「小」=0、「中」=1、「大」=2）。シンプルですが、順序のないカテゴリに使うと数値の大小関係をモデルが誤って解釈する恐れがあります。順序性のあるカテゴリ（サイズのS/M/Lなど）には適しています。",
  },
  {
    id: "ml-094",
    category: "データサイエンス力",
    question: "標準化（Standardization）の説明として正しいものはどれか？",
    choices: [
      "データを0から1の範囲に変換する手法",
      "データの平均を0、標準偏差を1にスケーリングする手法",
      "データの欠損値を補完する手法",
      "データをカテゴリに分割する手法",
    ],
    correctIndex: 1,
    explanation:
      "標準化は各特徴量から平均を引き、標準偏差で割ることで、平均0・標準偏差1の分布に変換する手法です。正規化（Min-Max Scaling）がデータを0〜1に変換するのに対し、標準化は外れ値の影響を受けにくい特徴があります。SVMやk-NNなど、距離を使うアルゴリズムではスケーリングが重要です。",
  },
  {
    id: "ml-095",
    category: "データサイエンス力",
    question: "特徴量スケーリングが必要な理由として正しいものはどれか？",
    choices: [
      "データのカテゴリ変数を処理するため",
      "特徴量のスケール（単位や範囲）の違いにより特定の特徴量が学習に過大な影響を与えることを防ぐため",
      "データの欠損値を補完するため",
      "データのサンプル数を増やすため",
    ],
    correctIndex: 1,
    explanation:
      "特徴量のスケールが大きく異なる場合（例：年齢は0〜100、年収は0〜数千万）、スケールの大きい特徴量がモデルの学習に過度な影響を与えます。特に距離を使う手法（k-NN、SVM）や勾配降下法ベースの手法では、スケーリングにより学習が安定し、精度が向上します。",
  },
  // ===== 強化学習・その他 =====
  {
    id: "ml-096",
    category: "データサイエンス力",
    question: "強化学習の基本的な構成要素として正しい組み合わせはどれか？",
    choices: [
      "入力データ・出力データ・損失関数",
      "エージェント・環境・報酬",
      "訓練データ・テストデータ・検証データ",
      "エンコーダ・デコーダ・注意機構",
    ],
    correctIndex: 1,
    explanation:
      "強化学習は、エージェント（学習者）が環境の中で行動を選択し、その行動に対する報酬を受け取りながら、累積報酬を最大化する方策（行動指針）を学習するフレームワークです。ゲームAI、ロボット制御、自動運転などに応用されています。",
  },
  {
    id: "ml-097",
    category: "データサイエンス力",
    question: "強化学習が教師あり学習と異なる点として最も適切なものはどれか？",
    choices: [
      "正解ラベルの代わりに、行動の結果として得られる報酬信号に基づいて学習する",
      "データに正解ラベルが付与されている",
      "データのクラスタリングを行う",
      "データの次元削減を行う",
    ],
    correctIndex: 0,
    explanation:
      "教師あり学習は「この入力に対してこれが正解」という明示的なラベルから学習しますが、強化学習は正解が直接与えられず、試行錯誤を通じて環境から得られる報酬信号に基づいて最適な行動を学習します。報酬は即座に得られるとは限らず、遅延報酬の考慮も重要です。",
  },
  {
    id: "ml-098",
    category: "データサイエンス力",
    question: "転移学習の説明として正しいものはどれか？",
    choices: [
      "あるタスクで学習したモデルの知識を、別の関連するタスクに活用する手法",
      "複数のモデルの予測を組み合わせて最終予測を行う手法",
      "学習データを別のデータ形式に変換する手法",
      "モデルを異なるプログラミング言語に移植する手法",
    ],
    correctIndex: 0,
    explanation:
      "転移学習は、大規模データで学習済みのモデル（例：ImageNetで学習したCNN）の知識を、データが少ない別のタスクに活用する手法です。ゼロから学習する必要がなく、少ないデータでも高い性能が得られることが多いです。特に画像認識や自然言語処理で広く利用されています。",
  },
  {
    id: "ml-099",
    category: "データサイエンス力",
    question: "アンサンブル学習の説明として正しいものはどれか？",
    choices: [
      "1つの強力なモデルだけで予測を行う手法",
      "複数のモデルを組み合わせて、単一モデルよりも高い予測性能を目指す手法",
      "データを1つのクラスタにまとめる手法",
      "モデルのパラメータ数を削減する手法",
    ],
    correctIndex: 1,
    explanation:
      "アンサンブル学習は、複数のモデルの予測を統合することで、個々のモデルよりも高い予測精度や安定性を得る手法の総称です。代表的な手法にバギング（ランダムフォレストなど）、ブースティング（XGBoost、LightGBMなど）、スタッキングがあります。「三人寄れば文殊の知恵」のような考え方です。",
  },
  {
    id: "ml-100",
    category: "データサイエンス力",
    question: "正規化（Min-Maxスケーリング）の説明として正しいものはどれか？",
    choices: [
      "データの平均を0、標準偏差を1に変換する手法",
      "データを0から1（または任意の範囲）にスケーリングする手法",
      "データの対数をとる変換手法",
      "データをカテゴリごとにグループ分けする手法",
    ],
    correctIndex: 1,
    explanation:
      "正規化（Min-Maxスケーリング）は、データの最小値を0、最大値を1に変換し、すべてのデータを0〜1の範囲に収める手法です。計算式は (x - min) / (max - min) です。標準化と同様に特徴量のスケールを揃える目的で使われますが、外れ値の影響を受けやすいという特徴があります。",
  },
];
