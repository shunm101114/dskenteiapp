import type { StudyTopic } from "../types";

export const studyTopicsDsExtra: StudyTopic[] = [
  // ========================================
  // 基礎統計（6 topics: ds-topic-01 ~ 06）
  // ========================================
  {
    id: "ds-topic-01",
    category: "データサイエンス力",
    subcategory: "基礎統計",
    title: "代表値と散布度",
    points: [
      "平均値: データの合計をデータ数で割った値",
      "中央値: データを大きさ順に並べた中央の値（外れ値に強い）",
      "最頻値: 最も頻度が高い値",
      "分散: 各データと平均の差の2乗の平均",
      "標準偏差: 分散の平方根（元データと同じ単位）",
      "四分位範囲（IQR）: Q3 - Q1で中央50%のばらつき",
    ],
    formula: "母分散 σ² = Σ(xi - x̄)² / n、不偏分散 s² = Σ(xi - x̄)² / (n-1)、標準偏差 σ = √σ²",
    detail:
      "代表値はデータの中心傾向を示し、散布度はばらつきの大きさを示します。平均値は外れ値の影響を受けやすいため、所得データなどでは中央値が適切です。箱ひげ図はQ1、中央値、Q3と外れ値を可視化し、データの分布の形状を一目で把握できます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-02",
    category: "データサイエンス力",
    subcategory: "基礎統計",
    title: "相関係数と共分散",
    points: [
      "ピアソンの相関係数: 線形関係の強さと方向（-1〜1）",
      "スピアマンの順位相関: 順位に基づく単調関係の指標",
      "共分散: 2変数の連動の方向を示す（単位に依存）",
      "相関と因果の区別: 相関があっても因果関係があるとは限らない",
      "擬似相関: 第三の変数（交絡因子）による見かけの相関",
    ],
    formula: "r = Σ(xi-x̄)(yi-ȳ) / √[Σ(xi-x̄)²·Σ(yi-ȳ)²]",
    detail:
      "相関係数は2変数間の線形関係の強さを-1から1で表します。|r|>0.7で強い相関、0.4〜0.7で中程度、0.4未満で弱い相関と判断する目安があります。アイスクリームの売上と水難事故件数に相関がある例は、気温という交絡因子による擬似相関の典型です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-03",
    category: "データサイエンス力",
    subcategory: "基礎統計",
    title: "信頼区間と区間推定",
    points: [
      "点推定: 母数を1つの値で推定（例：標本平均）",
      "区間推定: 母数が含まれる範囲を確率付きで推定",
      "95%信頼区間: 同じ方法で繰り返し推定すると95%が母数を含む",
      "標準誤差: 標本平均のばらつき（標準偏差/√n）",
      "サンプルサイズ増→信頼区間が狭くなる（精度向上）",
    ],
    formula: "95%CI: x̄ ± 1.96 × σ/√n",
    detail:
      "信頼区間は母数の推定の不確実性を示す重要な概念です。95%信頼区間とは「同じ手続きを100回繰り返すと、約95回は真の母数を含む区間が得られる」という意味です。サンプルサイズnを大きくすると標準誤差が小さくなり信頼区間が狭まります。ビジネスでは点推定値だけでなく信頼区間も報告することで、推定の確からしさを伝えられます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-04",
    category: "データサイエンス力",
    subcategory: "基礎統計",
    title: "カイ二乗検定",
    points: [
      "適合度検定: 観測度数が期待度数と合致するか検定",
      "独立性検定: 2つのカテゴリ変数が独立かどうか検定",
      "クロス集計表（分割表）: カテゴリ間の関係を表で整理",
      "自由度: (行数-1) × (列数-1)",
      "期待度数: 独立と仮定した場合の理論的な頻度",
    ],
    formula: "χ² = Σ(Oi - Ei)² / Ei",
    detail:
      "カイ二乗検定はカテゴリカルデータの分析に使う代表的な検定です。例えば「性別と商品の購入有無に関連があるか」を検定できます。帰無仮説は「2つの変数は独立（関連がない）」で、χ²統計量が大きいほど独立でない（関連がある）ことを示します。期待度数が5未満のセルが多い場合はフィッシャーの正確検定を使います。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-05",
    category: "データサイエンス力",
    subcategory: "基礎統計",
    title: "t検定の基礎",
    points: [
      "1標本t検定: 標本平均が特定の値と異なるか検定",
      "対応のある2標本t検定: 同一対象の前後比較",
      "対応のない2標本t検定: 2群の平均の差を検定",
      "帰無仮説: 2群の平均に差がない（μ1 = μ2）",
      "等分散の仮定: Welchのt検定は等分散を仮定しない",
    ],
    formula: "t = (x̄1 - x̄2) / √(s1²/n1 + s2²/n2)",
    detail:
      "t検定は2群の平均値に統計的に有意な差があるかを判定する基本的な検定手法です。例えば新旧デザインのWebサイトでの平均滞在時間の差を検定できます。対応のあるt検定はビフォーアフターの比較、対応のないt検定は異なるグループ間の比較に使います。等分散が仮定できない場合はWelchのt検定が推奨されます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-06",
    category: "データサイエンス力",
    subcategory: "基礎統計",
    title: "ノンパラメトリック検定",
    points: [
      "正規性を仮定しない検定手法の総称",
      "ウィルコクソン順位和検定: 対応なし2群の比較（t検定の代替）",
      "ウィルコクソン符号順位検定: 対応あり2群の比較",
      "クラスカル・ウォリス検定: 3群以上の比較（ANOVAの代替）",
      "適用場面: 順序データ、歪んだ分布、サンプルが小さい場合",
    ],
    detail:
      "ノンパラメトリック検定はデータの分布に仮定を置かない検定手法で、正規分布に従わないデータや順序尺度のデータに適用できます。パラメトリック検定（t検定等）と比べて検出力はやや低いですが、分布の仮定が成り立たない場合により頑健な結果が得られます。アンケートの5段階評価など順序データの分析でよく使われます。",
    relatedQuestionIds: [],
  },

  // ========================================
  // 確率・数理統計（7 topics: ds-topic-07 ~ 13）
  // ========================================
  {
    id: "ds-topic-07",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "大数の法則と中心極限定理",
    points: [
      "大数の法則: 試行回数を増やすと標本平均は母平均に収束",
      "中心極限定理: 標本平均の分布はnが大きいと正規分布に近づく",
      "母集団分布に依存しない: どんな分布でも標本平均は正規分布に",
      "n≥30が目安: 中心極限定理が適用できるサンプルサイズの目安",
      "統計的推論の理論的基盤: 信頼区間や仮説検定の根拠",
    ],
    detail:
      "大数の法則と中心極限定理は統計学の根幹をなす定理です。大数の法則はサイコロを何千回も振ると平均が3.5に近づく現象の理論的説明です。中心極限定理は母集団がどのような分布でも、十分大きなサンプルの平均値は正規分布に従うことを保証し、信頼区間や仮説検定の理論的根拠となっています。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-08",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "条件付き確率と独立性",
    points: [
      "条件付き確率: P(A|B) = P(A∩B) / P(B)",
      "独立事象: P(A∩B) = P(A)×P(B) のとき独立",
      "全確率の法則: 排反な事象で場合分けして確率を計算",
      "ベイズの定理の適用: 事前確率を事後確率に更新",
      "偽陽性問題: 検査精度が高くても有病率が低いと偽陽性が多い",
    ],
    detail:
      "条件付き確率は「Bが起きた条件の下でAが起きる確率」です。医療検査の例では、検査陽性の場合の実際の罹患確率（陽性的中率）は、疾病の有病率に大きく依存します。有病率0.1%の疾病で感度99%・特異度99%の検査を行うと、陽性的中率は約9%に過ぎません。この直感に反する結果はベイズの定理で説明できます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-09",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "微分・積分の基礎とデータサイエンスへの応用",
    points: [
      "微分: 関数の変化率（傾き）を求める操作",
      "偏微分: 多変数関数で1変数だけを変化させた微分",
      "勾配: 各変数の偏微分をベクトルにまとめたもの",
      "積分: 面積の計算、確率密度関数の面積=確率",
      "損失関数の最小化: 微分で勾配を求め最適なパラメータを探索",
    ],
    detail:
      "微分と積分は機械学習の理論的基盤です。損失関数を最小化するために勾配（偏微分のベクトル）を計算し、パラメータを更新する勾配降下法は、ほぼすべての機械学習アルゴリズムの最適化に使われています。積分は確率密度関数の面積として確率を計算する際に必要です。データサイエンティストには概念の理解が求められます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-10",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "情報理論の基礎",
    points: [
      "エントロピー: 不確実性・情報量の指標 H = -Σp·log₂(p)",
      "相互情報量: 2変数の依存関係の強さ",
      "KLダイバージェンス: 2つの確率分布の「距離」",
      "交差エントロピー: 分類モデルの損失関数に利用",
      "情報利得: 決定木の分岐基準（エントロピーの減少量）",
    ],
    formula: "H(X) = -Σ p(x) log₂ p(x)",
    detail:
      "情報理論はシャノンが創始した分野で、データサイエンスに広く応用されています。エントロピーはデータの不確実性を定量化し、決定木アルゴリズムの分岐基準として使われます。交差エントロピーはニューラルネットワークの分類タスクの標準的な損失関数です。KLダイバージェンスは生成モデル（VAEなど）の学習に使われます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-11",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "固有値・固有ベクトルとその応用",
    points: [
      "固有値・固有ベクトル: Av = λv を満たすλとv",
      "主成分分析: 共分散行列の固有値＝各主成分の分散",
      "固有値が大きい＝データのばらつきをより多く説明する方向",
      "スペクトル分解: 行列を固有値と固有ベクトルで分解",
      "グラフのラプラシアン: スペクトルクラスタリングに応用",
    ],
    detail:
      "固有値と固有ベクトルは線形代数の中心概念で、データサイエンスでは主成分分析（PCA）、スペクトルクラスタリング、推薦システム（SVD）など多くの手法の基礎です。PCAではデータの共分散行列の固有値が大きい順に主成分を選ぶことで、高次元データの次元削減を行います。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-12",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "ベイズ統計学の基礎",
    points: [
      "事前分布: 観測前のパラメータに関する信念",
      "尤度: 観測データがパラメータのもとで得られる確率",
      "事後分布: 事前分布と尤度から更新されたパラメータの分布",
      "MAP推定: 事後分布の最頻値をパラメータ推定値とする",
      "MCMC: 事後分布からのサンプリング（マルコフ連鎖モンテカルロ法）",
    ],
    formula: "P(θ|D) ∝ P(D|θ) × P(θ)",
    detail:
      "ベイズ統計学は事前知識とデータを組み合わせてパラメータの分布を推定する枠組みです。頻度論統計がパラメータを固定値として扱うのに対し、ベイズ統計はパラメータ自体に確率分布を仮定します。データが少ない場合でも事前知識を活用でき、不確実性を確率分布として自然に表現できるのが強みです。スパムフィルタや医療診断支援など幅広く応用されています。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-13",
    category: "データサイエンス力",
    subcategory: "確率・数理統計",
    title: "多重検定と検定の多重性問題",
    points: [
      "多重検定問題: 検定を繰り返すと偽陽性の確率が上昇",
      "ファミリーワイズ誤り率: 1つ以上の偽陽性が出る確率",
      "ボンフェローニ補正: 有意水準をα/k に厳しくする",
      "FDR（偽発見率）: 偽陽性の割合を制御するBenjamini-Hochberg法",
      "事前の仮説設定: 探索的分析と確証的分析の区別が重要",
    ],
    detail:
      "有意水準5%で20回検定を行うと、すべて帰無仮説が正しくても少なくとも1回は有意になる確率は約64%です。ゲノム解析や大規模A/Bテストでは多数の検定を同時に行うため、多重検定補正が不可欠です。ボンフェローニ補正は保守的すぎる場合があり、FDR制御（Benjamini-Hochberg法）がより実用的な選択です。",
    relatedQuestionIds: [],
  },

  // ========================================
  // 多変量解析・応用統計（7 topics: ds-topic-14 ~ 20）
  // ========================================
  {
    id: "ds-topic-14",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "重回帰分析の実践",
    points: [
      "多重共線性: 説明変数同士が強く相関しVIFが大きくなる問題",
      "VIF（分散拡大係数）: 10以上は多重共線性の疑い",
      "決定係数R²: モデルの当てはまりの良さ（0〜1）",
      "自由度調整済みR²: 変数が増えてもペナルティを課す",
      "残差分析: 正規性、等分散性、独立性の仮定を確認",
    ],
    detail:
      "重回帰分析は複数の説明変数から目的変数を予測する基本手法です。多重共線性があると回帰係数の推定が不安定になるため、VIFで検出し変数選択やリッジ回帰で対処します。R²だけでなく残差プロットで仮定の逸脱を確認することが重要です。ダミー変数を使えばカテゴリカルデータも説明変数に含められます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-15",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "ロジスティック回帰分析",
    points: [
      "2値分類のための回帰モデル（確率を0〜1に出力）",
      "シグモイド関数: 線形結合を0〜1の確率に変換",
      "オッズ比: 説明変数1単位増加時のオッズの変化倍率",
      "最尤推定: 尤度関数を最大化してパラメータを推定",
      "解釈性: 各変数の影響を係数とオッズ比で定量的に説明可能",
    ],
    formula: "P(Y=1) = 1 / (1 + e^-(β₀+β₁x₁+...))",
    detail:
      "ロジスティック回帰は分類タスクの最も基本的なモデルで、解釈性が高いため実務で広く使われます。例えば顧客の解約予測で、各特徴量（利用頻度、契約期間等）が解約確率にどう影響するかをオッズ比で定量的に説明できます。機械学習の複雑なモデルと比較してベースラインとしても重要です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-16",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "因子分析",
    points: [
      "潜在因子: 直接観測できない背後の要因を抽出",
      "因子負荷量: 各観測変数と因子の関連の強さ",
      "因子回転: バリマックス回転で因子の解釈性を向上",
      "PCAとの違い: PCAは分散最大化、因子分析は潜在構造の推定",
      "適用例: アンケート分析で質問項目の背後にある因子を発見",
    ],
    detail:
      "因子分析は多数の観測変数の背後にある少数の潜在的な因子を推定する手法です。例えば100問のアンケートから「満足度」「信頼度」「利便性」のような潜在因子を抽出できます。主成分分析と似ていますが、因子分析は観測変数が潜在因子から生成されるという統計モデルに基づいており、因子の解釈に重点を置きます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-17",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "判別分析",
    points: [
      "線形判別分析（LDA）: クラス間分散を最大化する射影を求める",
      "フィッシャーの判別: 2群を最もよく分離する1次元に射影",
      "二次判別分析（QDA）: クラスごとに異なる共分散を許容",
      "次元削減としての利用: 教師あり次元削減（PCAは教師なし）",
      "マハラノビス距離: 共分散を考慮した距離尺度",
    ],
    detail:
      "判別分析はクラスを最もよく分離する方向を見つける多変量解析の手法です。PCAが分散最大の方向を求めるのに対し、LDAはクラスラベルの情報を利用してクラス間の分離が最大になる方向を求めます。少数次元に射影するため次元削減としても利用できます。マハラノビス距離はデータの相関構造を考慮した距離で、異常検知にも応用されます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-18",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "時系列分析の基礎",
    points: [
      "トレンド: 長期的な上昇・下降傾向",
      "季節性: 周期的に繰り返すパターン（月次、曜日等）",
      "定常性: 平均と分散が時間によらず一定",
      "自己相関: 時点tと時点t-kの相関",
      "ARIMAモデル: 自己回帰、差分、移動平均の組み合わせ",
    ],
    detail:
      "時系列分析はデータの時間的な構造を分析する手法群です。データをトレンド、季節性、残差に分解するのが基本アプローチです。ARIMAモデルは定常時系列に対する古典的な予測手法で、非定常データは差分をとって定常化します。近年はProphet（Meta開発）やLSTMなどの深層学習ベースの手法も広く使われています。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-19",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "傾向スコアマッチング",
    points: [
      "傾向スコア: 処置を受ける確率をロジスティック回帰等で推定",
      "マッチング: 傾向スコアが近い処置群と対照群のペアを作成",
      "IPW（逆確率重み付け）: 傾向スコアの逆数で重み付け",
      "観察研究でのバイアス軽減: ランダム化できない場合の因果推論",
      "仮定: 強く無視可能な割り当て（測定した交絡因子で十分）",
    ],
    detail:
      "傾向スコアマッチングは観察研究（ランダム化実験ができない場合）で因果効果を推定する手法です。例えば新しいマーケティング施策の効果を、施策を受けた群と受けなかった群で比較する際、両群の背景の違い（交絡因子）を傾向スコアで調整します。ただし測定されていない交絡因子は制御できないという限界があります。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-20",
    category: "データサイエンス力",
    subcategory: "多変量解析・応用統計",
    title: "混合モデルとベイジアンモデリング",
    points: [
      "混合正規分布: 複数の正規分布の重ね合わせでデータを表現",
      "EMアルゴリズム: 潜在変数を含むモデルのパラメータ推定",
      "階層ベイズモデル: グループ間の情報を共有して推定精度を向上",
      "事前分布の選択: 無情報事前分布と情報事前分布",
      "モデル比較: WAIC、LOOなどの情報量基準で比較",
    ],
    detail:
      "混合モデルはデータが複数の部分集団から生成されていると仮定するモデルで、クラスタリングや密度推定に使われます。EMアルゴリズムは潜在変数の期待値計算（E-step）とパラメータ最適化（M-step）を交互に繰り返してパラメータを推定します。階層ベイズモデルは個別データが少ない場合でもグループ全体の情報を借りて推定精度を向上させる強力な手法です。",
    relatedQuestionIds: [],
  },

  // ========================================
  // 機械学習（10 topics: ds-topic-21 ~ 30）
  // ========================================
  {
    id: "ds-topic-21",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "決定木とルールベース学習",
    points: [
      "分岐基準: 情報利得（エントロピー）、ジニ不純度",
      "剪定（プルーニング）: 過学習を防ぐために木を刈り込む",
      "解釈性: if-then ルールとして人間が読める",
      "CART: 二分木による分類・回帰アルゴリズム",
      "アンサンブルの基本要素: ランダムフォレストやXGBoostの構成要素",
    ],
    detail:
      "決定木は特徴量の条件分岐によりデータを分割する直感的なアルゴリズムです。ジニ不純度は「ランダムに選んだサンプルが誤分類される確率」を表し、これが最も減少する分岐点を選びます。単体では過学習しやすいですが、ランダムフォレストやGBDTの構成要素として使うことで高い性能を発揮します。解釈性の高さから、意思決定の説明が求められるビジネス場面で重宝されます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-22",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "サポートベクターマシン（SVM）",
    points: [
      "マージン最大化: クラス間の境界マージンを最大にする分離超平面",
      "サポートベクター: 境界に最も近いデータ点",
      "カーネルトリック: 非線形分離をカーネル関数で実現",
      "RBFカーネル: 最も広く使われるカーネル関数",
      "正則化パラメータC: マージンの広さとミス許容のトレードオフ",
    ],
    detail:
      "SVMはクラス間のマージンを最大化する分離超平面を見つけるアルゴリズムです。カーネルトリックにより、元の特徴量空間では線形分離できないデータも高次元空間で線形分離します。少数サンプルでも高い汎化性能を発揮しますが、大規模データでは学習に時間がかかる傾向があります。テキスト分類や画像認識で広く使われてきました。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-23",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "k近傍法（k-NN）",
    points: [
      "怠惰学習: 学習フェーズがなく、予測時に全データを参照",
      "距離関数: ユークリッド距離、マンハッタン距離、コサイン類似度",
      "kの選択: 小さいkは過学習、大きいkは過少適合の傾向",
      "次元の呪い: 高次元では距離の差が意味をなくなる",
      "分類と回帰: k個の近傍の多数決（分類）or 平均（回帰）",
    ],
    detail:
      "k-NNは最も直感的な機械学習アルゴリズムの一つです。予測時に入力データに最も近いk個の訓練データを見つけ、多数決（分類）や平均（回帰）で予測します。モデルの学習が不要なため実装が簡単ですが、データ量が多いと予測に時間がかかります。特徴量のスケーリング（標準化）が結果に大きく影響するため前処理が重要です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-24",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "勾配ブースティング（GBDT）",
    points: [
      "ブースティング: 弱学習器を逐次的に追加して精度を向上",
      "残差学習: 前のモデルの残差を次のモデルが学習",
      "XGBoost: 正則化と高速化を組み込んだGBDT実装",
      "LightGBM: ヒストグラムベースで高速な学習が可能",
      "テーブルデータのコンペで最も強いアルゴリズムの一つ",
    ],
    detail:
      "勾配ブースティング決定木（GBDT）はテーブルデータの機械学習で最も強力な手法の一つです。浅い決定木を逐次的に追加し、前の段階で予測できなかった残差を次の木が学習します。XGBoostはKaggleコンペティションで圧倒的な成果を上げ、LightGBMはさらに高速な学習を実現しました。CatBoostはカテゴリ変数の自動処理に強みがあります。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-25",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "交差検証とモデル評価戦略",
    points: [
      "ホールドアウト法: データを訓練/テストに分割",
      "k分割交差検証: k個に分割し、各折をテストデータとして評価",
      "層化抽出: クラス比率を各分割で維持",
      "時系列分割: 未来のデータが訓練に漏れないよう時間順に分割",
      "リーケージ防止: テストデータの情報が訓練に漏れないよう注意",
    ],
    detail:
      "モデルの汎化性能を正しく評価するには適切な検証戦略が不可欠です。k分割交差検証は限られたデータを有効に使える標準的な手法です。時系列データでは未来のデータが過去の予測に使われるリーケージを防ぐため、時間順に分割する必要があります。データの前処理（標準化、特徴量生成等）もCV内で行わないとリーケージになるため注意が必要です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-26",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "混同行列と分類指標の深掘り",
    points: [
      "混同行列: TP, FP, TN, FN の4象限",
      "適合率（Precision）: 陽性予測のうち実際に陽性の割合",
      "再現率（Recall）: 実際の陽性のうち正しく予測できた割合",
      "F1スコア: 適合率と再現率の調和平均",
      "AUC-ROC: 閾値に依存しないモデル性能の総合指標",
    ],
    formula: "F1 = 2 × (Precision × Recall) / (Precision + Recall)",
    detail:
      "分類モデルの評価はAccuracyだけでは不十分です。不均衡データ（例：不正検知で陽性1%）ではすべて陰性と予測しても99%の精度になります。適合率は「陽性と予測したものの正確さ」、再現率は「陽性の見逃しの少なさ」を測ります。ビジネス上のコスト（見逃しのコスト vs 誤報のコスト）に応じて閾値を調整することが重要です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-27",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "欠損値の処理手法",
    points: [
      "MCAR / MAR / MNAR: 欠損メカニズムの3分類",
      "リストワイズ削除: 欠損のある行を削除（MCARなら妥当）",
      "平均値・中央値補完: 単純だがばらつきを過小評価",
      "多重補完法（MICE）: 欠損を確率モデルで補完し複数データを生成",
      "モデルベースの補完: k-NNやランダムフォレストで欠損を予測",
    ],
    detail:
      "欠損値の処理は分析結果に大きく影響するため、欠損メカニズムの理解が重要です。MCAR（完全にランダム）では削除しても偏りが生じませんが、MAR（他の変数に依存）やMNAR（欠損値自体に依存）では補完が必要です。多重補完法は欠損の不確実性を反映した複数のデータセットを生成し、より頑健な推定を行えます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-28",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "ハイパーパラメータチューニング",
    points: [
      "グリッドサーチ: パラメータの全組み合わせを網羅的に探索",
      "ランダムサーチ: ランダムにパラメータを選択（効率的）",
      "ベイズ最適化: 過去の結果から次に試すパラメータを賢く選択",
      "Optuna: Pythonの効率的なベイズ最適化ライブラリ",
      "早期打ち切り: 見込みのない試行を途中で中断",
    ],
    detail:
      "ハイパーパラメータはモデルの学習前に設定する値（学習率、木の深さ等）で、適切な調整がモデル性能を大きく左右します。グリッドサーチは確実ですが次元数が増えると計算量が爆発します。ランダムサーチは同じ計算時間でより広い空間を探索でき、実用的にはグリッドサーチ以上の性能を示すことが多いです。Optunaはベイズ最適化を使い効率的にパラメータを探索します。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-29",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "回帰モデルの評価指標",
    points: [
      "MSE（平均二乗誤差）: 外れ値に敏感な誤差指標",
      "RMSE: MSEの平方根（元データと同じ単位で解釈可能）",
      "MAE（平均絶対誤差）: 外れ値に頑健な誤差指標",
      "R²（決定係数）: モデルがデータの分散をどれだけ説明するか",
      "MAPE: 誤差を%で表示（スケール非依存）",
    ],
    formula: "RMSE = √(Σ(yi - ŷi)² / n)",
    detail:
      "回帰モデルの評価指標は目的に応じて使い分けます。RMSEは大きな誤差に敏感で、外れ値のペナルティを重視する場合に適します。MAEはすべての誤差を均等に扱い、外れ値の影響を受けにくいです。MAPEはスケールが異なるデータ間の比較に便利ですが、実測値が0に近い場合は使えません。複数の指標を組み合わせて総合的に評価することが推奨されます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-30",
    category: "データサイエンス力",
    subcategory: "機械学習",
    title: "転移学習とファインチューニング",
    points: [
      "転移学習: 事前学習済みモデルの知識を新しいタスクに活用",
      "ファインチューニング: 事前学習モデルの一部を新データで再学習",
      "特徴抽出: 事前学習モデルを固定し、最終層のみ学習",
      "ImageNet事前学習: 画像系モデルの標準的な初期化",
      "少量データでも高精度: 事前学習による初期値が有効",
    ],
    detail:
      "転移学習は大規模データで学習済みのモデルの知識を、データが少ない新しいタスクに転用する手法です。画像認識ではImageNetで事前学習したモデル（ResNet等）を医療画像分類などに活用し、少数の学習データでも高精度を実現できます。NLP分野ではBERTやGPTなどの事前学習済み言語モデルのファインチューニングが標準的なアプローチです。",
    relatedQuestionIds: [],
  },

  // ========================================
  // 深層学習・ニューラルネットワーク（9 topics: ds-topic-31 ~ 39）
  // ========================================
  {
    id: "ds-topic-31",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "活性化関数の種類と特徴",
    points: [
      "ReLU: max(0, x)。勾配消失しにくく最も広く使用される",
      "Sigmoid: 出力を0〜1に変換。2値分類の出力層に使用",
      "Softmax: 多クラス分類の出力層で確率分布に変換",
      "tanh: 出力を-1〜1に変換。RNN等で使用",
      "GELU: GPT/BERTで使用される滑らかなReLU変種",
    ],
    detail:
      "活性化関数はニューラルネットワークに非線形性を導入する関数です。ReLUは計算が単純で勾配消失問題を軽減するため、隠れ層の標準的な選択です。ただしReLUはx<0で勾配が0になる「死んだニューロン」問題があり、Leaky ReLUやGELUで改善されます。出力層ではタスクに応じてSigmoid（2値分類）、Softmax（多クラス分類）、恒等関数（回帰）を使い分けます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-32",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "CNN（畳み込みニューラルネットワーク）",
    points: [
      "畳み込み層: フィルタでローカルな特徴（エッジ、テクスチャ）を抽出",
      "プーリング層: 空間方向のダウンサンプリング（Max/Average Pooling）",
      "特徴マップ: フィルタの数だけ特徴マップが生成される",
      "代表的モデル: LeNet → AlexNet → VGG → ResNet → EfficientNet",
      "残差接続（Skip Connection）: ResNetの深い層でも学習可能にする技術",
    ],
    detail:
      "CNNは画像認識の中核をなすアーキテクチャで、局所的な特徴を階層的に学習します。浅い層はエッジや色の特徴、深い層は物体の部品や全体構造を捉えます。ResNetの残差接続（Skip Connection）は100層以上の深いネットワークの学習を可能にした革新的な技術です。画像分類、物体検出、セグメンテーションなど幅広い画像タスクに応用されます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-33",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "RNN・LSTMと系列データ処理",
    points: [
      "RNN: 隠れ状態を通じて系列データの時間依存性を学習",
      "勾配消失/爆発: RNNの長期依存性学習の困難さ",
      "LSTM: 忘却ゲート・入力ゲート・出力ゲートで長期記憶を制御",
      "GRU: LSTMを簡略化した2ゲート構造",
      "Seq2Seq: エンコーダ-デコーダ構造で系列を別の系列に変換",
    ],
    detail:
      "RNNは時系列データやテキストなどの系列データを処理するネットワークです。基本的なRNNは長い系列で勾配消失問題が起きるため、LSTMやGRUが開発されました。LSTMはゲート機構により「何を覚え、何を忘れるか」を学習し、長期依存性を捉えられます。現在はTransformerに置き換えられつつありますが、リアルタイム処理や少リソース環境ではRNN系が有利な場合があります。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-34",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "Transformer アーキテクチャ",
    points: [
      "Self-Attention: 入力系列の全位置間の関連度を並列計算",
      "Multi-Head Attention: 異なる表現部分空間で複数のAttentionを実行",
      "位置エンコーディング: 系列の順序情報を埋め込み",
      "エンコーダ-デコーダ構造: 元の論文の翻訳モデル",
      "スケーラビリティ: 並列計算が可能でGPUとの相性が良い",
    ],
    detail:
      "Transformerは2017年の論文「Attention Is All You Need」で提案されたアーキテクチャで、現在のAI革命の基盤です。Self-Attentionにより入力の全要素間の関係を一度に計算でき、RNNの逐次処理よりも並列化に優れています。BERT（エンコーダのみ）、GPT（デコーダのみ）、T5（エンコーダ-デコーダ）など、タスクに応じた派生モデルが生まれました。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-35",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "GAN（敵対的生成ネットワーク）",
    points: [
      "Generator: ノイズから偽データを生成するネットワーク",
      "Discriminator: 入力が本物か偽物かを判別するネットワーク",
      "敵対的学習: GeneratorとDiscriminatorが互いに競い合い向上",
      "応用: 画像生成、データ拡張、超解像、スタイル変換",
      "課題: モード崩壊、学習の不安定さ",
    ],
    detail:
      "GANは2つのネットワークを敵対的に学習させることでリアルなデータを生成するモデルです。偽札製造者（Generator）と鑑定士（Discriminator）に例えられます。StyleGANは高品質な顔画像生成で注目を集めました。現在は拡散モデル（Stable Diffusion等）に主役の座を譲りつつありますが、データ拡張やドメイン適応など引き続き活用されています。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-36",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "正則化と深層学習の学習テクニック",
    points: [
      "ドロップアウト: 学習時にランダムにニューロンを無効化",
      "バッチ正規化: 層の出力を正規化し学習を安定化",
      "早期終了: 検証誤差が悪化し始めたら学習を停止",
      "データ拡張: 画像の回転・反転等でデータを増やす",
      "学習率スケジューリング: 学習の進行に合わせて学習率を調整",
    ],
    detail:
      "深層学習ではパラメータ数が膨大なため過学習防止が重要です。ドロップアウトはランダムにニューロンを無効化することでアンサンブル効果を得ます。バッチ正規化は各層の入力分布を安定させ、より高い学習率の使用と高速な収束を可能にします。これらのテクニックを組み合わせることで、深いネットワークでも効率的に学習できます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-37",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "オートエンコーダ",
    points: [
      "エンコーダ: 入力を低次元の潜在表現に圧縮",
      "デコーダ: 潜在表現から入力を再構成",
      "ボトルネック: 中間層の次元を入力より小さくして情報を圧縮",
      "VAE（変分オートエンコーダ）: 潜在空間に確率分布を仮定し生成が可能",
      "応用: 次元削減、異常検知、ノイズ除去、データ生成",
    ],
    detail:
      "オートエンコーダは入力を再構成するよう学習するニューラルネットワークです。ボトルネック構造により入力の本質的な特徴を低次元の潜在表現として抽出します。異常検知では正常データで学習し、異常データの再構成誤差が大きくなることを利用します。VAEは潜在空間を連続的な確率分布として学習するため、新たなデータの生成が可能です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-38",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "物体検出とセグメンテーション",
    points: [
      "物体検出: 画像内の物体の位置（バウンディングボックス）とクラスを同時に推定",
      "YOLO: リアルタイム物体検出モデル（1回の推論で検出）",
      "セマンティックセグメンテーション: ピクセル単位でクラスを分類",
      "インスタンスセグメンテーション: 物体ごとに個別にマスクを生成",
      "アンカーフリー手法: 事前定義ボックス不要の最新手法",
    ],
    detail:
      "物体検出は画像分類の発展形で、「何が」「どこに」あるかを同時に推定します。YOLOはリアルタイム処理が可能で、自動運転、監視カメラ、製造業の外観検査などで広く利用されています。セマンティックセグメンテーションはピクセル単位の分類で、医療画像の腫瘍領域検出や自動運転の道路認識に使われます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-39",
    category: "データサイエンス力",
    subcategory: "深層学習・ニューラルネットワーク",
    title: "拡散モデル（Diffusion Model）",
    points: [
      "順拡散過程: データに段階的にノイズを付加",
      "逆拡散過程: ノイズからデータを段階的に復元（生成）",
      "代表モデル: Stable Diffusion、DALL-E、Midjourney",
      "テキスト条件付き生成: テキストプロンプトから画像を生成",
      "ControlNet: 構図やポーズなどの条件付き制御",
    ],
    detail:
      "拡散モデルは画像生成AIの現在の主流です。データにノイズを段階的に付加する過程の逆を学習し、ランダムノイズから高品質な画像を生成します。GANと比べて学習が安定しており、多様な出力を生成できます。Stable Diffusionはオープンソースで公開され、テキストから画像を生成するText-to-Imageの民主化に貢献しました。",
    relatedQuestionIds: [],
  },

  // ========================================
  // 自然言語処理・画像認識（8 topics: ds-topic-40 ~ 47）
  // ========================================
  {
    id: "ds-topic-40",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "テキストの前処理と特徴量化",
    points: [
      "トークン化: テキストを単語やサブワードに分割",
      "ストップワード除去: 「の」「は」等の高頻度低情報語を除去",
      "ステミング/レンマタイズ: 語を基本形に変換",
      "Bag of Words: 単語の出現頻度でベクトル化",
      "TF-IDF: 文書内頻度×逆文書頻度で単語の重要度を計算",
    ],
    formula: "TF-IDF = TF(t,d) × log(N / DF(t))",
    detail:
      "テキストデータは非構造化データであり、機械学習で扱うには数値ベクトルへの変換が必要です。Bag of Wordsは単語の出現頻度で文書をベクトル化する最もシンプルな手法です。TF-IDFは多くの文書に出現する一般的な単語の重みを下げ、特定の文書に特徴的な単語の重みを上げます。近年はBERTなどの事前学習モデルによる文脈を考慮した表現（Embedding）が主流です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-41",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "単語埋め込み（Word Embedding）",
    points: [
      "分布仮説: 同じ文脈に出現する単語は似た意味を持つ",
      "Word2Vec: Skip-gram / CBOWで単語をベクトル化",
      "ベクトル演算: king - man + woman ≒ queen のような意味演算",
      "FastText: サブワード（文字n-gram）を考慮した埋め込み",
      "コサイン類似度: 単語ベクトル間の意味的類似度を計算",
    ],
    detail:
      "単語埋め込みは単語を数百次元の密なベクトルで表現する技術で、自然言語処理を大きく発展させました。Word2Vecは大量のテキストから単語の意味的な関係をベクトル空間上に学習します。「王」-「男性」+「女性」≒「女王」のようなベクトル演算が可能です。FastTextはサブワードを使うため、未知語にも対応でき、日本語のような膠着語にも有効です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-42",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "BERTと事前学習言語モデル",
    points: [
      "BERT: 双方向Transformerエンコーダの事前学習モデル",
      "マスク言語モデル: 文中の一部をマスクし予測する事前学習",
      "ファインチューニング: 下流タスク（分類、QA等）に適応",
      "文ベクトル: [CLS]トークンの表現で文全体をベクトル化",
      "日本語BERT: 東北大BERT、cl-tohoku/bert-base-japanese",
    ],
    detail:
      "BERT（Bidirectional Encoder Representations from Transformers）は2018年にGoogleが発表した事前学習言語モデルで、NLPの多くのタスクで性能を大幅に向上させました。大量のテキストで事前学習し、少量のラベル付きデータでファインチューニングすることで、感情分析、固有表現抽出、質問応答など様々なタスクに適用できます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-43",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "固有表現認識と情報抽出",
    points: [
      "固有表現認識（NER）: テキストから人名・組織名・日付等を抽出",
      "系列ラベリング: 各トークンにBIO形式のラベルを付与",
      "関係抽出: エンティティ間の関係（所属、位置等）を抽出",
      "知識グラフ: 抽出した情報をグラフ構造に構造化",
      "spaCy / GiNZA: 実用的なNLPライブラリ（日本語対応）",
    ],
    detail:
      "固有表現認識は非構造化テキストから構造化された情報を自動抽出する技術です。ニュース記事から企業名と業績を抽出、契約書から当事者と期限を抽出など、実務で広く使われます。BERTベースのモデルにより高精度なNERが可能になりました。抽出した情報を知識グラフとして構造化することで、検索や推論に活用できます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-44",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "機械翻訳と文書要約",
    points: [
      "ニューラル機械翻訳: Seq2SeqモデルやTransformerで翻訳",
      "Attention機構: 翻訳時に入力文の関連部分に注目",
      "抽出型要約: 原文から重要な文を選んで要約",
      "生成型要約: LLMが原文を理解し新たな文で要約",
      "BLEU/ROUGE: 翻訳・要約の自動評価指標",
    ],
    detail:
      "機械翻訳はNLPの代表的な応用で、Google翻訳やDeepLなどのサービスで実用化されています。Transformerの登場により翻訳精度が飛躍的に向上しました。文書要約は大量の文書から効率的に情報を得る手段として重要性が増しており、LLMにより人間に近い品質の生成型要約が可能になっています。BLEUは翻訳、ROUGEは要約の自動評価に使われます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-45",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "画像分類の実践手法",
    points: [
      "データ拡張: 回転、反転、色調変更、ランダムクロップ等",
      "事前学習モデル: ResNet、EfficientNet、ViT（Vision Transformer）",
      "転移学習: ImageNet事前学習→ドメイン固有タスクにファインチューニング",
      "Grad-CAM: モデルが注目した領域のヒートマップ可視化",
      "混合精度学習: FP16/FP32を混合して学習を高速化",
    ],
    detail:
      "画像分類の実践ではデータの収集・前処理から、事前学習モデルの選定、ファインチューニング、評価・運用までの一連の流れを理解することが重要です。Vision Transformer（ViT）は画像をパッチに分割してTransformerで処理する手法で、CNNに匹敵する性能を示しています。Grad-CAMでモデルの判断根拠を可視化し、正しい特徴を学習しているか確認することも実務では重要です。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-46",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "音声認識と音声処理",
    points: [
      "音声波形→メルスペクトログラム→テキストへ変換",
      "Whisper: OpenAIの多言語音声認識モデル",
      "End-to-End モデル: 従来のパイプラインを単一モデルに統合",
      "話者分離: 複数話者の音声を個別に認識",
      "音声合成（TTS）: テキストから自然な音声を生成",
    ],
    detail:
      "音声認識は音声データからテキストを生成する技術です。OpenAIのWhisperは大規模データで学習されたオープンソースモデルで、日本語を含む多言語の高精度な文字起こしが可能です。音声認識と自然言語処理を組み合わせることで、議事録の自動作成、コールセンターの応対分析、音声アシスタントなどの応用が実現されています。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-47",
    category: "データサイエンス力",
    subcategory: "自然言語処理・画像認識",
    title: "マルチモーダルAIの応用",
    points: [
      "CLIP: テキストと画像を同じ埋め込み空間に対応付け",
      "Vision-Language Model: 画像を理解し言語で説明するモデル",
      "GPT-4V / Gemini: テキスト・画像・音声を統合して処理",
      "画像キャプション生成: 画像の内容を自動で文章化",
      "Visual QA: 画像に関する質問に自然言語で回答",
    ],
    detail:
      "マルチモーダルAIは複数のモダリティ（テキスト、画像、音声等）を統合して処理するモデルです。CLIPはテキストと画像を共通のベクトル空間に埋め込むことで、テキストで画像を検索したり、画像をテキストで分類したりできます。GPT-4VやGeminiは画像を入力として受け取り、テキストで応答できるマルチモーダルLLMとして広く活用されています。",
    relatedQuestionIds: [],
  },

  // ========================================
  // AI応用・最新技術（3 topics: ds-topic-48 ~ 50）
  // ========================================
  {
    id: "ds-topic-48",
    category: "データサイエンス力",
    subcategory: "AI応用・最新技術",
    title: "RAGとLLMの知識拡張",
    points: [
      "RAG: Retrieval-Augmented Generation（検索拡張生成）",
      "外部知識の検索→プロンプトに追加→LLMが回答を生成",
      "Embedding + ベクトルDB: 類似文書の高速検索",
      "ハルシネーション軽減: 根拠付きの回答で信頼性向上",
      "チャンク分割: 文書を適切なサイズに分割して検索精度を向上",
    ],
    detail:
      "RAGは大規模言語モデルの知識の限界を外部データベースで補完する技術です。社内文書やFAQをベクトルDBに格納し、ユーザーの質問に関連する情報を検索してLLMに渡すことで、最新かつ正確な回答を生成します。LLMの学習データに含まれない情報にも対応でき、ハルシネーション（事実に基づかない回答）を軽減できます。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-49",
    category: "データサイエンス力",
    subcategory: "AI応用・最新技術",
    title: "プロンプトエンジニアリング",
    points: [
      "Zero-Shot: 例示なしで指示のみでタスクを実行",
      "Few-Shot: 少数の入出力例を示してタスクを誘導",
      "Chain-of-Thought: 思考過程を段階的に示させて推論精度を向上",
      "システムプロンプト: LLMの役割や制約を事前に設定",
      "プロンプトテンプレート: 再利用可能な定型プロンプト",
    ],
    detail:
      "プロンプトエンジニアリングはLLMに適切な指示を与えて望ましい出力を得る技術です。Chain-of-Thought（CoT）プロンプティングは「ステップバイステップで考えて」と指示することで、複雑な推論タスクの精度を大幅に向上させます。プロンプトの設計はLLM活用の鍵であり、データサイエンティストの新しい必須スキルとなっています。",
    relatedQuestionIds: [],
  },
  {
    id: "ds-topic-50",
    category: "データサイエンス力",
    subcategory: "AI応用・最新技術",
    title: "強化学習の基礎",
    points: [
      "エージェント: 環境と相互作用して学習する主体",
      "報酬: 行動の良し悪しを示すフィードバック信号",
      "方策（Policy）: 状態から行動への写像",
      "Q学習: 状態-行動の価値関数を学習",
      "RLHF: 人間のフィードバックで言語モデルを調整（ChatGPT等）",
    ],
    detail:
      "強化学習はエージェントが環境と試行錯誤を繰り返し、累積報酬を最大化する方策を学習する枠組みです。囲碁AI（AlphaGo）、ロボット制御、ゲームAIなどで大きな成果を上げています。近年はRLHF（Reinforcement Learning from Human Feedback）としてLLMの学習に応用され、ChatGPTなどの対話AIの品質向上に貢献しています。",
    relatedQuestionIds: [],
  },
];
