import type { StudyTopic } from "../types";

export const studyTopicsDeExtra: StudyTopic[] = [
  {
    id: "de-topic-26",
    category: "データエンジニアリング力",
    title: "NoSQLデータベースの基礎",
    points: [
      "キーバリュー型: Redis、DynamoDB。高速なルックアップに適する",
      "ドキュメント型: MongoDB。JSONライクな柔軟なスキーマ",
      "カラム型: Cassandra、HBase。大量データの書き込みに強い",
      "グラフ型: Neo4j。ノードとエッジで関係性をモデル化",
      "CAP定理: 一貫性・可用性・分断耐性の3つを同時に満たせない",
    ],
    detail:
      "NoSQLデータベースはRDBの制約を超えてスケーラビリティや柔軟性を提供します。用途に応じてキーバリュー型（セッション管理、キャッシュ）、ドキュメント型（コンテンツ管理、カタログ）、カラム型（時系列データ、IoT）、グラフ型（ソーシャルネットワーク、推薦）を選択します。CAP定理により、分散システムでは一貫性（Consistency）、可用性（Availability）、分断耐性（Partition tolerance）のうち最大2つしか保証できません。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-27",
    category: "データエンジニアリング力",
    title: "データウェアハウスとデータマート",
    points: [
      "DWH: 全社データを統合・蓄積する分析用データベース",
      "データマート: 特定部門・目的に特化したDWHのサブセット",
      "スタースキーマ: 中央のファクトテーブルと周囲のディメンションテーブル",
      "スノーフレークスキーマ: ディメンションをさらに正規化した構造",
      "OLAP vs OLTP: 分析処理（読み中心）と業務処理（書き中心）の違い",
    ],
    detail:
      "データウェアハウス（DWH）は業務システムからデータを集約し、意思決定支援のための分析基盤を提供します。スタースキーマは売上などの数値データを持つファクトテーブルを中心に、日付・顧客・商品などのディメンションテーブルを配置する設計パターンです。OLAPは大量データの集計・分析に最適化され、OLTPはトランザクション処理に最適化されています。代表的なDWHにはAmazon Redshift、Google BigQuery、Snowflakeがあります。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-28",
    category: "データエンジニアリング力",
    title: "データモデリングの基礎",
    points: [
      "概念モデル: ビジネス要件をエンティティと関連で表現",
      "論理モデル: 属性・データ型・制約を定義（DBMS非依存）",
      "物理モデル: 特定DBMSに合わせたテーブル・インデックス設計",
      "ER図: エンティティ（箱）、リレーションシップ（線）、属性（楕円）",
      "カーディナリティ: 1対1、1対多、多対多の関連の種類",
    ],
    detail:
      "データモデリングはデータの構造と関係を体系的に設計するプロセスです。概念モデルで全体像を把握し、論理モデルで詳細を定義し、物理モデルで実装に落とし込みます。ER図（Entity-Relationship Diagram）はデータモデルを視覚化する標準的な手法です。良いデータモデルはデータの整合性を保ち、クエリの効率を向上させ、将来の拡張にも対応しやすくなります。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-29",
    category: "データエンジニアリング力",
    title: "データレイクハウスアーキテクチャ",
    points: [
      "データレイクとDWHの長所を統合した新しいアーキテクチャ",
      "Delta Lake: Sparkベースのオープンソーステーブルフォーマット",
      "Apache Iceberg: 大規模分析テーブル向けオープンフォーマット",
      "ACID トランザクションをデータレイク上で実現",
      "スキーマエボリューション: テーブル構造の変更を安全に管理",
    ],
    detail:
      "データレイクハウスはデータレイクのスケーラビリティとコスト効率に、DWHのデータ管理機能（ACIDトランザクション、スキーマ管理、データ品質）を組み合わせたアーキテクチャです。Delta LakeやApache Icebergなどのテーブルフォーマットにより、オブジェクトストレージ上でもトランザクション管理やタイムトラベル（過去時点のデータ参照）が可能になります。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-30",
    category: "データエンジニアリング力",
    title: "Linuxコマンドラインの基礎",
    points: [
      "ファイル操作: ls, cp, mv, rm, mkdir, cat, less",
      "テキスト処理: grep, sed, awk, sort, uniq, wc",
      "パイプ（|）: コマンドの出力を次のコマンドの入力に渡す",
      "リダイレクト: > で上書き、>> で追記、< で入力",
      "パーミッション: rwx（読み・書き・実行）、chmod で変更",
    ],
    detail:
      "Linuxコマンドラインはデータエンジニアリングの基本スキルです。サーバー管理、ログ分析、データ前処理などで日常的に使用します。パイプとリダイレクトを組み合わせることで、複雑なデータ処理をワンライナーで実行できます。例えば「cat access.log | grep ERROR | sort | uniq -c | sort -rn | head」で最も多いエラーを特定できます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-31",
    category: "データエンジニアリング力",
    title: "正規表現の基礎",
    points: [
      "メタ文字: . は任意の1文字、* は0回以上、+ は1回以上の繰り返し",
      "文字クラス: [0-9] は数字、[a-zA-Z] は英字、\\d, \\w, \\s",
      "アンカー: ^ は行頭、$ は行末",
      "グループ化: () でキャプチャ、(?:) で非キャプチャグループ",
      "Python: re.search(), re.findall(), re.sub() で活用",
    ],
    detail:
      "正規表現はテキストパターンマッチングの強力なツールで、データクレンジングやログ解析で頻繁に使用します。例えばメールアドレスの検証、電話番号の抽出、日付フォーマットの変換などに活用できます。Pythonのreモジュールやpandasのstr.contains()、str.extract()と組み合わせることで、大量のテキストデータから必要な情報を効率的に抽出できます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-32",
    category: "データエンジニアリング力",
    title: "BIツールとデータ可視化基盤",
    points: [
      "Tableau: ドラッグ&ドロップで直感的なビジュアライゼーション",
      "Power BI: Microsoft製、Excel連携に強い",
      "Looker: SQLベースのデータモデリング（LookML）",
      "セルフサービスBI: 専門知識なしにビジネスユーザーが分析可能",
      "ダッシュボード: KPIを一覧表示し意思決定を支援",
    ],
    detail:
      "BIツールはデータウェアハウスやデータベースに接続し、データをグラフ・チャート・ダッシュボードで可視化します。セルフサービスBIの普及により、データエンジニアはデータの整備と提供に注力し、ビジネスユーザーが自ら分析できる環境を構築することが重要です。データエンジニアはBIツールが効率的にクエリできるよう、データマートの設計や集計テーブルの準備を行います。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-33",
    category: "データエンジニアリング力",
    title: "メッセージキューとイベント駆動アーキテクチャ",
    points: [
      "Apache Kafka: 大規模分散メッセージストリーミング基盤",
      "RabbitMQ: AMQPプロトコル準拠の汎用メッセージブローカー",
      "Producer/Consumer モデル: 送信者と受信者を疎結合に",
      "Pub/Sub: トピックに対して複数のサブスクライバーが購読",
      "イベントソーシング: 状態変更をイベントの系列として記録",
    ],
    detail:
      "メッセージキューはシステム間のデータ連携を非同期・疎結合に実現する仕組みです。Apache Kafkaは高スループット・低レイテンシのストリーミング処理に適し、リアルタイムデータパイプラインやイベント駆動アーキテクチャの中核を担います。Pub/Subパターンでは、1つのイベント（例：注文完了）を在庫管理、通知、分析など複数のシステムが独立して処理できます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-34",
    category: "データエンジニアリング力",
    title: "オブジェクトストレージの基礎",
    points: [
      "Amazon S3: AWSの代表的オブジェクトストレージ",
      "バケット: オブジェクトを格納するコンテナ（名前空間）",
      "ストレージクラス: 標準、低頻度アクセス、アーカイブで料金が異なる",
      "ブロックストレージとの違い: ファイル単位で操作、HTTP APIでアクセス",
      "データレイクの基盤として非構造化データの格納に最適",
    ],
    detail:
      "オブジェクトストレージは大量の非構造化データ（画像、動画、ログ、CSV等）を安価に保存できるストレージです。Amazon S3、Google Cloud Storage、Azure Blob Storageが代表的です。データレイクの基盤として利用され、ParquetやORCなどの列指向フォーマットと組み合わせることで、分析クエリの効率を大幅に向上させることができます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-35",
    category: "データエンジニアリング力",
    title: "データガバナンスの基礎",
    points: [
      "データオーナーシップ: データの品質・管理に責任を持つ役割",
      "データスチュワード: データの利用ルールやポリシーを運用管理",
      "データリネージ: データの発生源から最終利用までの経路を追跡",
      "データ品質指標: 正確性、完全性、一貫性、適時性、一意性",
      "コンプライアンス: GDPR、個人情報保護法への準拠",
    ],
    detail:
      "データガバナンスは組織全体でデータの品質、セキュリティ、利用ルールを管理する枠組みです。データリネージによりデータがどこから来てどう加工されたかを追跡でき、障害時の影響範囲特定や品質問題の原因究明に役立ちます。GDPR（EU一般データ保護規則）や日本の個人情報保護法に準拠するためにも、データの分類、アクセス制御、保持期間の管理が重要です。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-36",
    category: "データエンジニアリング力",
    title: "Webスクレイピングとデータ収集",
    points: [
      "HTML解析: BeautifulSoup（Python）でDOM要素を取得",
      "ブラウザ自動操作: Selenium、Playwrightで動的ページに対応",
      "Scrapy: Python製の大規模スクレイピングフレームワーク",
      "robots.txt: サイトのクロールルールを確認・遵守",
      "API利用の優先: スクレイピングよりAPI提供があればそちらを使う",
    ],
    detail:
      "Webスクレイピングは公開Webサイトからデータを自動収集する技術です。PythonのBeautifulSoupやScrapyが広く使われます。ただし、利用規約の確認、robots.txtの遵守、過度なアクセスの回避など倫理的・法的な配慮が必要です。APIが提供されている場合はAPIを優先し、スクレイピングは最終手段として使います。収集したデータはクレンジング後にデータパイプラインに組み込みます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-37",
    category: "データエンジニアリング力",
    title: "文字コードとエンコーディング",
    points: [
      "ASCII: 英数字を7ビットで表現（128文字）",
      "UTF-8: 可変長（1〜4バイト）で世界中の文字を表現、最も普及",
      "Shift_JIS: 日本語の従来エンコーディング、レガシーシステムで残存",
      "文字化け: エンコードとデコードの不一致で発生",
      "BOM（Byte Order Mark）: ファイル先頭のエンコーディング識別子",
    ],
    detail:
      "文字コードの理解はデータ処理で避けて通れません。現在はUTF-8が国際標準として広く使われていますが、日本のレガシーシステムではShift_JISやEUC-JPが残っています。CSVファイルの読み込みやDB間のデータ移行で文字化けが発生した場合、エンコーディングの不一致を疑います。Pythonではopen()のencoding引数やpandasのread_csv(encoding=)で指定できます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-38",
    category: "データエンジニアリング力",
    title: "バッチ処理とジョブスケジューリング",
    points: [
      "バッチ処理: まとまったデータを一括で処理する方式",
      "ストリーム処理との違い: レイテンシとスループットのトレードオフ",
      "cron: Linux標準のジョブスケジューラ（分・時・日・月・曜日）",
      "Apache Airflow: DAGベースのワークフロー管理ツール",
      "冪等性: 同じ処理を何度実行しても同じ結果になる設計",
    ],
    detail:
      "バッチ処理は日次・時次などの定期間隔でデータを一括処理する方式で、ETLパイプラインの基本です。cronは最もシンプルなスケジューラですが、タスク間の依存関係管理やエラーハンドリングには限界があります。Apache AirflowやDigdagなどのワークフローエンジンでは、DAG（有向非巡回グラフ）でタスクの依存関係を定義し、リトライ、アラート、ログ管理を統合的に行えます。冪等性の確保はバッチ処理の信頼性に不可欠です。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-39",
    category: "データエンジニアリング力",
    title: "データの匿名化と仮名化",
    points: [
      "匿名化: 個人を特定できないよう不可逆に変換",
      "仮名化: 識別子を別の値に置換（対応表で復元可能）",
      "k-匿名性: 同じ属性の組み合わせが少なくともk件存在",
      "差分プライバシー: 統計的ノイズを加えて個人特定を防止",
      "マスキング: データの一部を隠す（例: クレジットカード番号の*処理）",
    ],
    detail:
      "個人データの利活用には適切な匿名化・仮名化が必要です。匿名化は元の個人を特定できないレベルまで変換する不可逆処理で、仮名化は対応表を分離管理することで必要時に復元可能な処理です。k-匿名性は同じ属性値の組み合わせが少なくともk人分存在することを保証し、個人の特定リスクを下げます。GDPRや個人情報保護法の遵守にはこれらの技術の理解が重要です。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-40",
    category: "データエンジニアリング力",
    title: "ログ管理と分析基盤",
    points: [
      "構造化ログ: JSON形式で出力し解析を容易にする",
      "ELKスタック: Elasticsearch + Logstash + Kibanaによるログ分析",
      "Fluentd/Fluent Bit: ログの収集・転送・変換エージェント",
      "ログレベル: DEBUG < INFO < WARN < ERROR < FATAL",
      "集中ログ管理: 分散システムのログを一箇所に集約",
    ],
    detail:
      "ログはシステムの動作状況を把握し、障害対応や性能分析に不可欠なデータです。ELKスタック（Elasticsearch、Logstash、Kibana）やGrafana Lokiが広く使われます。FluentdやFluent Bitでアプリケーションログを収集し、Elasticsearchに格納、Kibanaで可視化するのが典型的な構成です。構造化ログ（JSON形式）にすることで、フィールドごとの検索や集計が容易になります。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-41",
    category: "データエンジニアリング力",
    title: "データバックアップとディザスタリカバリ",
    points: [
      "フルバックアップ: 全データを丸ごとコピー",
      "増分バックアップ: 前回から変更された分のみ保存",
      "RPO（目標復旧時点）: どの時点までデータを戻せるか",
      "RTO（目標復旧時間）: 障害発生から復旧までの許容時間",
      "レプリケーション: リアルタイムにデータを別拠点に複製",
    ],
    detail:
      "データのバックアップとディザスタリカバリ（DR）は事業継続に不可欠です。RPOが短いほどデータ損失が少なく、RTOが短いほど早く復旧できますが、コストとのトレードオフがあります。クラウドではマルチリージョンレプリケーションにより高い可用性を実現できます。3-2-1ルール（3つのコピー、2種類の媒体、1つはオフサイト）がバックアップの基本原則です。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-42",
    category: "データエンジニアリング力",
    title: "エッジコンピューティングとIoTデータ",
    points: [
      "エッジ処理: データ発生源の近くで前処理し通信量を削減",
      "IoTプロトコル: MQTT（軽量メッセージング）、HTTP/REST",
      "タイムシリーズDB: InfluxDB、TimescaleDBで時系列データを効率管理",
      "ストリーム処理: リアルタイムにセンサーデータを分析",
      "フォグコンピューティング: エッジとクラウドの中間層で処理を分散",
    ],
    detail:
      "IoTデバイスから生成される大量のセンサーデータは、すべてをクラウドに送ると帯域とコストが問題になります。エッジコンピューティングではデバイスやゲートウェイ側でフィルタリング・集約・異常検知を行い、必要なデータだけをクラウドに送信します。時系列データベース（InfluxDB等）は時刻をキーとしたデータの格納・検索に特化しており、IoTデータの管理に適しています。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-43",
    category: "データエンジニアリング力",
    title: "Rプログラミングの基礎",
    points: [
      "データフレーム: R最基本のデータ構造（行×列の表形式）",
      "tidyverse: dplyr, ggplot2, tidyrなどのパッケージ群",
      "dplyr: filter, select, mutate, summarise, group_by で操作",
      "ggplot2: 文法に基づいた柔軟なデータ可視化",
      "R Markdown / Quarto: 分析レポートの再現可能な文書化",
    ],
    detail:
      "Rは統計解析とデータ可視化に強みを持つプログラミング言語です。tidyverseパッケージ群を使えば、パイプ演算子（|>）でデータの読み込みから加工・可視化までを直感的に記述できます。ggplot2はグラフィックス文法に基づく可視化ライブラリで、レイヤーを重ねて複雑なチャートを構築できます。Pythonと並びデータサイエンス分野で広く使われています。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-44",
    category: "データエンジニアリング力",
    title: "データプロファイリングと探索的データ分析",
    points: [
      "データプロファイリング: カラムの型、NULL率、分布、外れ値を自動調査",
      "基本統計量: 件数、平均、中央値、最小、最大、標準偏差",
      "ユニーク数とカーディナリティ: 値の種類数がJOIN性能に影響",
      "pandas-profiling / ydata-profiling: 自動レポート生成ツール",
      "データの偏り: クラスの不均衡、欠損パターンの確認",
    ],
    detail:
      "データプロファイリングはデータの特徴を事前に把握するプロセスで、ETL設計やモデリングの前段階として重要です。各カラムの型、NULL率、値の分布、外れ値を確認し、データ品質の問題を早期に発見します。Pythonではpandasのdescribe()やinfo()、ydata-profilingによる自動レポート生成が便利です。カーディナリティ（値の種類数）はインデックス設計やJOIN戦略にも影響します。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-45",
    category: "データエンジニアリング力",
    title: "サーバーレスアーキテクチャとデータ処理",
    points: [
      "AWS Lambda: イベント駆動でコードを実行（サーバー管理不要）",
      "Cloud Functions: GCPのサーバーレス実行環境",
      "Azure Functions: Azureのサーバーレスコンピューティング",
      "従量課金: 実行時間とメモリ使用量に基づく料金体系",
      "コールドスタート: 初回起動時のレイテンシに注意",
    ],
    detail:
      "サーバーレスアーキテクチャではインフラ管理が不要で、イベント（ファイルアップロード、API呼び出し、スケジュール等）をトリガーにコードが実行されます。小規模なデータ変換、ファイル処理、API連携などに適しています。ただしコールドスタートによるレイテンシ、実行時間の制限（通常15分以内）、ステートレスな設計が必要など制約もあります。大規模バッチ処理には向かない場合があります。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-46",
    category: "データエンジニアリング力",
    title: "マスターデータ管理（MDM）",
    points: [
      "マスターデータ: 顧客、商品、拠点など組織横断の基幹データ",
      "トランザクションデータ: 売上、注文など業務で発生する実績データ",
      "名寄せ: 同一エンティティの重複レコードを統合",
      "ゴールデンレコード: 統合された唯一の正しいレコード",
      "MDMツール: Informatica MDM、Talendなど",
    ],
    detail:
      "マスターデータ管理（MDM）は組織全体で共通の基幹データ（顧客、商品、従業員等）の一貫性と正確性を保つ取り組みです。システムごとに異なる形式で管理された顧客データを名寄せし、ゴールデンレコード（信頼できる単一のレコード）を作成します。MDMが不十分だと、同一顧客への重複営業、不正確な売上集計、データ分析の信頼性低下などの問題が発生します。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-47",
    category: "データエンジニアリング力",
    title: "テストとデータ品質の自動検証",
    points: [
      "単体テスト: 個々のデータ変換ロジックの正しさを検証",
      "dbt test: SQLベースのデータ品質テスト（NOT NULL、ユニーク等）",
      "Great Expectations: Pythonのデータ検証フレームワーク",
      "スキーマテスト: カラム名、データ型、制約の一致を検証",
      "データ契約: プロデューサーとコンシューマー間の品質合意",
    ],
    detail:
      "データパイプラインにもソフトウェアと同様のテスト文化が重要です。dbtではモデルに対してnot_null、unique、accepted_values、relationshipsなどのテストを宣言的に定義できます。Great Expectationsはデータに対する期待値（Expectation）を定義し、パイプラインの各段階で自動検証を行います。データ契約はアップストリームとダウンストリーム間でスキーマや品質基準を合意する仕組みです。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-48",
    category: "データエンジニアリング力",
    title: "データの圧縮とファイルフォーマット",
    points: [
      "行指向: CSV、JSON。行単位の読み書きに適する",
      "列指向: Parquet、ORC。分析クエリ（特定列の集計）に高速",
      "圧縮アルゴリズム: gzip、snappy、zstd（速度と圧縮率のバランス）",
      "Avro: スキーマ内蔵のバイナリ形式、スキーマ進化に対応",
      "列指向 + 圧縮: 同一カラムは類似値が多く高い圧縮率を実現",
    ],
    detail:
      "データの保存形式はパイプラインの性能とコストに大きく影響します。CSV/JSONは可読性が高いですが、大規模データにはParquet（列指向・圧縮済み）が推奨されます。列指向フォーマットは必要なカラムだけを読み込むため、SELECT句で少数カラムを取得するクエリが高速になります。Snappyは圧縮速度が速く、gzipは圧縮率が高く、zstdはそのバランスが良い選択です。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-49",
    category: "データエンジニアリング力",
    title: "dbtによるデータ変換",
    points: [
      "dbt: SQLベースのデータ変換ツール（ELTのT部分）",
      "モデル: SELECT文で変換を定義し、テーブル/ビューとして実体化",
      "ref関数: モデル間の依存関係を宣言し自動でDAGを構築",
      "マクロ: Jinja テンプレートでSQLを動的に生成",
      "ドキュメント: スキーマ定義をYAMLで記述し自動ドキュメント化",
    ],
    detail:
      "dbt（data build tool）はデータウェアハウス内のデータ変換をSQLで管理するツールで、ELTアプローチの中核を担います。各変換をモデル（SQLファイル）として定義し、ref()で依存関係を宣言すると自動的にDAGが構築され、正しい順序で実行されます。バージョン管理（Git）、テスト、ドキュメントがデータ変換に統合されるため、ソフトウェアエンジニアリングのベストプラクティスをデータパイプラインに適用できます。",
    relatedQuestionIds: [],
  },
  {
    id: "de-topic-50",
    category: "データエンジニアリング力",
    title: "ネットワークの基礎知識",
    points: [
      "TCP/IP: インターネット通信の基盤プロトコル体系",
      "HTTP/HTTPS: Webの通信プロトコル、HTTPSはTLSで暗号化",
      "DNS: ドメイン名をIPアドレスに変換する仕組み",
      "VPN/VPC: 安全な仮想ネットワーク環境を構築",
      "ファイアウォール: 不正な通信を遮断しネットワークを保護",
    ],
    detail:
      "ネットワークの基礎知識はクラウド環境でのデータ基盤構築に不可欠です。TCP/IPはインターネット通信の基盤で、HTTPはその上で動作するWebプロトコルです。クラウドではVPC（仮想プライベートクラウド）でネットワークを分離し、セキュリティグループやファイアウォールでアクセスを制御します。データベースやストレージへのアクセス制御にはこれらの知識が必要です。",
    relatedQuestionIds: [],
  },
];
