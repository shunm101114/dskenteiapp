import type { StudyTopic } from "../types";

export const studyTopics: StudyTopic[] = [
  // ===== データサイエンス力（数学・統計） =====
  {
    id: "math-topic-01",
    category: "データサイエンス力",
    title: "正規分布と標準偏差",
    points: [
      "正規分布は平均μ・分散σ²で決まる左右対称の釣鐘型分布",
      "平均±1σに約68%、±2σに約95%、±3σに約99.7%のデータが含まれる（68-95-99.7ルール）",
      "標準正規分布はμ=0, σ=1に標準化した正規分布",
      "中心極限定理により、標本平均の分布は標本サイズが大きいと正規分布に近づく",
    ],
    formula: "Z = (X - μ) / σ（標準化の公式）",
    detail:
      "正規分布（ガウス分布）は自然界や社会現象で最も広く観察される確率分布です。身長、テストの点数、測定誤差など多くの現象が正規分布に従います。標準偏差σはデータのばらつきの大きさを表し、分散σ²の正の平方根です。データを標準化（Zスコア変換）すると標準正規分布表を使って確率を求められます。",
    relatedQuestionIds: ["math-001", "math-004", "math-006"],
  },
  {
    id: "math-topic-02",
    category: "データサイエンス力",
    title: "ベイズの定理と条件付き確率",
    points: [
      "条件付き確率 P(A|B) は「Bが起きた下でAが起きる確率」",
      "ベイズの定理で事前確率から事後確率を計算できる",
      "P(A|B) = P(B|A)×P(A) / P(B)",
      "迷惑メールフィルタ、医療診断など幅広く応用される",
    ],
    formula: "P(A|B) = P(B|A) × P(A) / P(B)",
    detail:
      "ベイズの定理は、ある事象の事前確率（Prior）と新しいデータ（尤度）を組み合わせて事後確率（Posterior）を求める公式です。例えば病気の検査で陽性が出た場合、実際に病気である確率を計算できます。ベイズ推定は機械学習のナイーブベイズ分類器やベイズ最適化など多方面で活用されています。",
    relatedQuestionIds: ["math-002", "math-020"],
  },
  {
    id: "math-topic-03",
    category: "データサイエンス力",
    title: "仮説検定とp値",
    points: [
      "帰無仮説（H₀）と対立仮説（H₁）を設定して統計的に判断する",
      "第1種の過誤（α）: 帰無仮説が真なのに棄却する誤り",
      "第2種の過誤（β）: 帰無仮説が偽なのに棄却しない誤り",
      "p値が有意水準α未満であれば帰無仮説を棄却する",
      "t検定は2群の平均値の差、カイ二乗検定はカテゴリカルデータの独立性を検定",
    ],
    detail:
      "仮説検定は「差がない・効果がない」という帰無仮説を立て、データからその仮説が棄却できるかを判断する手法です。p値は帰無仮説が正しい場合に観測データ以上に極端な結果が得られる確率であり、帰無仮説が正しい確率ではないことに注意が必要です。有意水準は通常5%（α=0.05）が使われます。",
    relatedQuestionIds: ["math-005", "math-008", "math-012", "math-018"],
  },
  {
    id: "math-topic-04",
    category: "データサイエンス力",
    title: "回帰分析と相関",
    points: [
      "相関係数は-1～1の範囲で線形関係の強さと方向を表す",
      "共分散が正なら同じ方向に変動する傾向がある",
      "重回帰分析では多重共線性（説明変数間の高い相関）に注意",
      "相関関係は因果関係を意味しない",
    ],
    formula: "r = Cov(X,Y) / (σ_X × σ_Y)（相関係数の定義）",
    detail:
      "回帰分析は目的変数（従属変数）を説明変数（独立変数）で予測するモデルです。単回帰は1つの説明変数、重回帰は複数の説明変数を使います。最小二乗法で回帰係数を推定します。多重共線性がある場合はVIF（分散拡大要因）で検出し、変数の除外や正則化で対処します。",
    relatedQuestionIds: ["math-003", "math-013", "math-017"],
  },
  {
    id: "math-topic-05",
    category: "データサイエンス力",
    title: "行列・線形代数の基礎",
    points: [
      "逆行列が存在する条件は行列式(det)が0でないこと",
      "固有値・固有ベクトル: Av = λv を満たすλとv",
      "PCA（主成分分析）は共分散行列の固有値分解を利用",
      "行列の演算（積・転置・逆行列）は機械学習の基盤",
    ],
    formula: "Av = λv（固有値方程式）",
    detail:
      "線形代数はデータサイエンスの数学的基盤です。データは行列として表現され、行列演算を通じて変換・分析されます。固有値は行列による変換でスケーリングされる倍率、固有ベクトルはその方向を示します。主成分分析ではデータの分散が最大となる方向（第1主成分）が最大固有値に対応する固有ベクトルです。",
    relatedQuestionIds: ["math-009", "math-016"],
  },
  {
    id: "math-topic-06",
    category: "データサイエンス力",
    title: "確率分布と推定",
    points: [
      "ポアソン分布は単位時間あたりの稀な事象の回数をモデル化",
      "不偏分散はn-1で割る（ベッセルの補正）",
      "最尤推定法（MLE）は尤度関数を最大化するパラメータを求める",
      "対数変換は右に歪んだ分布を対称に近づける効果がある",
    ],
    detail:
      "確率分布はデータの生成過程をモデル化します。離散型にはベルヌーイ分布、二項分布、ポアソン分布など、連続型には正規分布、指数分布、一様分布などがあります。パラメータ推定では最尤推定法が広く使われ、対数尤度関数を微分して最大値を求めます。偏微分は多変数関数において1変数以外を定数として微分する操作で、勾配計算の基礎です。",
    relatedQuestionIds: ["math-010", "math-011", "math-014", "math-015", "math-019"],
  },

  // ===== データサイエンス力（機械学習） =====
  {
    id: "ml-topic-01",
    category: "データサイエンス力",
    title: "教師あり学習と教師なし学習",
    points: [
      "教師あり学習: 正解ラベル付きデータで学習（分類・回帰）",
      "教師なし学習: ラベルなしでデータの構造を発見（クラスタリング・次元削減）",
      "強化学習: 環境との相互作用で報酬を最大化",
      "代表手法 — 教師あり: ランダムフォレスト、SVM、NN / 教師なし: k-means、PCA",
    ],
    detail:
      "機械学習は大きく教師あり学習・教師なし学習・強化学習に分類されます。教師あり学習は入力と正解のペアから関数を学習し、未知のデータに対する予測を行います。教師なし学習はラベルなしのデータからパターンや構造を抽出します。強化学習はエージェントが環境と相互作用しながら、累積報酬を最大化する方策を学習します。",
    relatedQuestionIds: ["ml-001", "ml-006", "ml-019"],
  },
  {
    id: "ml-topic-02",
    category: "データサイエンス力",
    title: "ニューラルネットワークと深層学習",
    points: [
      "CNN（畳み込みNN）は画像認識に強い",
      "RNN（再帰型NN）は系列データ（時系列・テキスト）に適する",
      "Transformerは自己注意（Self-Attention）機構が中核",
      "勾配消失問題はReLUやBatch Normalizationで対策",
      "転移学習で事前学習済みモデルの知識を別タスクに活用",
    ],
    detail:
      "深層学習は多層のニューラルネットワークによる学習手法です。CNNは畳み込み層で画像の局所特徴を階層的に抽出します。RNNは隠れ状態を通じて過去の情報を保持し、LSTM/GRUで長期依存を扱えます。Transformerは自己注意機構により系列全体の関係を並列計算でき、BERTやGPTの基盤です。深い層で勾配が消失する問題にはReLU活性化関数やスキップ接続が有効です。",
    relatedQuestionIds: ["ml-011", "ml-012", "ml-014", "ml-015", "ml-020"],
  },
  {
    id: "ml-topic-03",
    category: "データサイエンス力",
    title: "アンサンブル学習",
    points: [
      "バギング: ブートストラップサンプルで複数モデルを学習し集約（例: ランダムフォレスト）",
      "ブースティング: 前のモデルの残差を次のモデルが学習する逐次的手法",
      "代表実装: XGBoost、LightGBM、CatBoost",
      "決定木のジニ不純度は全データが同一クラスのとき0",
    ],
    detail:
      "アンサンブル学習は複数のモデルを組み合わせて予測精度を向上させる手法です。バギング（Bootstrap Aggregating）はデータの復元抽出で多様なモデルを作り、分散を低減します。ブースティングは弱学習器を逐次的に追加し、バイアスを低減します。勾配ブースティングでは損失関数の勾配（残差）を次のモデルが学習し、テーブルデータのコンペティションで高い性能を示します。",
    relatedQuestionIds: ["ml-005", "ml-016", "ml-017"],
  },
  {
    id: "ml-topic-04",
    category: "データサイエンス力",
    title: "評価指標と交差検証",
    points: [
      "Precision（精度）= TP / (TP+FP): 陽性予測の正確さ",
      "Recall（再現率）= TP / (TP+FN): 実際の陽性の捕捉率",
      "F1スコア = 2×Precision×Recall / (Precision+Recall)",
      "ROC-AUC: 0.5でランダム予測と同等、1.0で完璧な分類",
      "k分割交差検証でモデルの汎化性能を安定的に評価",
    ],
    detail:
      "分類モデルの評価にはAccuracy（正解率）だけでなく、目的に応じた指標を使います。不均衡データではPrecisionとRecallのトレードオフが重要です。ROC曲線はFPRとTPRの関係をプロットし、AUCはその下の面積です。交差検証はデータをk分割してk回の学習・評価を行い、データの偏りによる評価のばらつきを軽減します。",
    relatedQuestionIds: ["ml-004", "ml-009", "ml-010"],
  },
  {
    id: "ml-topic-05",
    category: "データサイエンス力",
    title: "次元削減と正則化",
    points: [
      "PCA（主成分分析）はデータの分散が最大の方向に射影して次元を削減",
      "L1正則化（Lasso）は一部のパラメータを0にし特徴量選択の効果がある",
      "L2正則化（Ridge）はパラメータを0に近づけるが正確に0にはしない",
      "正則化は過学習を防ぐ効果がある",
    ],
    detail:
      "次元削減は高次元データを低次元に変換し、可視化や計算効率の向上、ノイズ除去に役立ちます。PCAは共分散行列の固有値分解により、分散が大きい方向（主成分）を抽出します。正則化は損失関数にパラメータの大きさに関するペナルティ項を加え、モデルの複雑さを抑制します。L1はスパースな解、L2は滑らかな解を促進します。",
    relatedQuestionIds: ["ml-013", "ml-018"],
  },
  {
    id: "ml-topic-06",
    category: "データサイエンス力",
    title: "過学習とバイアス-バリアンス",
    points: [
      "過学習: 訓練データに適合しすぎて未知データでの性能が低下する現象",
      "モデルが複雑→バイアス低下、バリアンス上昇（トレードオフ）",
      "対策: 正則化、ドロップアウト、早期打ち切り、データ拡張",
      "SVMのカーネルトリックは高次元空間で非線形分離を実現",
    ],
    detail:
      "バイアスはモデルの系統的な誤差（単純すぎて真の関係を捉えられない）、バリアンスはデータへの敏感さ（訓練データの変化で予測が大きく変わる）です。汎化性能はこの2つのバランスで決まります。過学習はバリアンスが高い状態で、正則化やドロップアウト、交差検証による適切なモデル選択で対処します。勾配降下法ではモメンタムが局所最適解からの脱出を助けます。",
    relatedQuestionIds: ["ml-002", "ml-003", "ml-007", "ml-008"],
  },

  // ===== データエンジニアリング力 =====
  {
    id: "de-topic-01",
    category: "データエンジニアリング力",
    title: "SQL基礎と集約",
    points: [
      "COUNT(column)はNULLを除外、COUNT(*)は全行をカウント",
      "GROUP BYで集約関数（SUM, AVG, COUNT等）と組み合わせてグループ集計",
      "INNER JOINは両テーブルの一致行のみ、LEFT JOINは左テーブルの全行を返す",
      "ウィンドウ関数 ROW_NUMBER() はパーティション内で連番を振る",
    ],
    detail:
      "SQLはデータ操作の基本言語です。SELECT文での集約ではGROUP BYで指定した列ごとにSUM/COUNT/AVGなどで集計します。JOINは複数テーブルの結合で、INNER/LEFT/RIGHT/FULL OUTER JOINがあります。ウィンドウ関数はOVER句でパーティションと順序を指定し、各行に対して集計値やランキングを計算できます。HAVINGはGROUP BY後の条件フィルタリングに使います。",
    relatedQuestionIds: ["de-001", "de-007", "de-011", "de-014"],
  },
  {
    id: "de-topic-02",
    category: "データエンジニアリング力",
    title: "データベース設計と正規化",
    points: [
      "第1正規形: 繰り返し項目の排除",
      "第2正規形: 部分関数従属の排除",
      "第3正規形: 推移的関数従属の排除",
      "ACID特性（原子性・一貫性・分離性・耐久性）がトランザクションの信頼性を保証",
      "NoSQL: キーバリュー型、ドキュメント型、カラム型、グラフ型",
    ],
    detail:
      "リレーショナルデータベースの正規化は、データの重複と更新異常を防ぐための設計手法です。正規化を進めるほどデータの整合性は高まりますが、JOINが増えて性能が低下する場合もあるため、適切なレベルを選択します。ACID特性はトランザクションの信頼性を保証する4つの性質で、特にIsolation（分離性）は並行トランザクション間の干渉を防ぎます。NoSQLはRDBMSと異なるデータモデルで、用途に応じて選択します。",
    relatedQuestionIds: ["de-004", "de-008", "de-010"],
  },
  {
    id: "de-topic-03",
    category: "データエンジニアリング力",
    title: "分散処理とストリーミング",
    points: [
      "Apache Sparkはインメモリ分散処理でHadoop MapReduceより高速",
      "Apache Kafkaはリアルタイムデータストリーミング基盤",
      "CAP定理: 一貫性・可用性・分断耐性の3つを同時に完全には満たせない",
      "Parquet形式は列指向で分析クエリに効率的",
    ],
    detail:
      "ビッグデータ処理ではデータを複数のノードに分散して並列処理します。Apache Sparkはメモリ上でデータを保持するため反復処理が高速です。Apache Kafkaは大量のリアルタイムデータ（ログ、イベント）を高スループットで処理する分散メッセージングシステムです。CAP定理は分散システム設計における基本的なトレードオフを示し、システムの要件に応じて重視する性質を選択します。",
    relatedQuestionIds: ["de-005", "de-015", "de-016", "de-018"],
  },
  {
    id: "de-topic-04",
    category: "データエンジニアリング力",
    title: "ETLとデータパイプライン",
    points: [
      "ETL: Extract（抽出）→ Transform（変換）→ Load（格納）",
      "データレイクは非構造化データも格納可能、DWHは構造化データ中心",
      "Apache Airflowはワークフローをプログラムで定義しスケジュール管理",
      "データメッシュは各ドメインチームがデータの所有権を持つ分散アプローチ",
    ],
    detail:
      "ETLはデータソースからの抽出、ビジネスルールに基づく変換、目的のストアへの格納という3段階のプロセスです。データレイクは多様な形式のデータをそのまま蓄積でき、データウェアハウスは分析用に構造化されたデータを格納します。Airflowは DAG（有向非巡回グラフ）でタスクの依存関係を定義し、パイプラインの実行・監視を自動化します。データメッシュは中央集権的なデータチームへの依存を減らす新しいアーキテクチャです。",
    relatedQuestionIds: ["de-002", "de-003", "de-013", "de-020"],
  },
  {
    id: "de-topic-05",
    category: "データエンジニアリング力",
    title: "API・データ形式とセキュリティ",
    points: [
      "REST API: GET(取得)、POST(作成)、PUT(更新)、DELETE(削除)",
      "SQLインジェクション対策にはプリペアドステートメントが最も有効",
      "Dockerはアプリと依存関係をコンテナとして隔離・再現可能にする",
      "Gitのブランチ戦略でコードレビューとCI/CDによる品質チェックを確保",
    ],
    detail:
      "REST APIはHTTPメソッドでリソースを操作するアーキテクチャスタイルです。セキュリティではSQLインジェクション対策としてプリペアドステートメント（パラメータ化クエリ）を使い、ユーザー入力がSQL文として解釈されることを防ぎます。Dockerコンテナは開発・本番環境の差異を解消し、再現可能なデプロイメントを実現します。Gitではフィーチャーブランチからプルリクエスト経由で統合することで品質を維持します。",
    relatedQuestionIds: ["de-006", "de-012", "de-017", "de-019"],
  },
  {
    id: "de-topic-06",
    category: "データエンジニアリング力",
    title: "データ品質とクレンジング",
    points: [
      "データクレンジング: 重複除去、欠損値補完、外れ値対処、表記ゆれ統一",
      "架空のデータを自動生成することはクレンジングに含まれない",
      "データの品質が低いとモデルの精度に直接影響する（GIGO原則）",
      "データプロファイリングで品質の現状を可視化する",
    ],
    detail:
      "データクレンジングは分析前にデータの品質を向上させるプロセスです。重複レコードの除去、欠損値の補完（平均値、中央値、最頻値、予測モデルなど）、外れ値の検出と対処、表記ゆれの統一などを行います。「Garbage In, Garbage Out（GIGO）」の原則通り、データ品質が低ければどんなに優れたモデルでも正確な結果は得られません。",
    relatedQuestionIds: ["de-009"],
  },

  // ===== ビジネス力 =====
  {
    id: "biz-topic-01",
    category: "ビジネス力",
    title: "CRISP-DMとプロジェクト管理",
    points: [
      "CRISP-DMの6フェーズ: ビジネス理解→データ理解→データ準備→モデリング→評価→展開",
      "最初にビジネス上の課題と目的を明確にすることが最重要",
      "PoC（概念実証）で実現可能性を小規模に検証してからスケールする",
      "アジャイル開発は短いサイクルで反復的に開発・フィードバックを繰り返す",
    ],
    detail:
      "CRISP-DM（Cross-Industry Standard Process for Data Mining）はデータ分析プロジェクトの標準的なプロセスモデルです。最初のフェーズ「ビジネス理解」で課題と目的を明確にすることが成功の鍵です。PoCは本格開発前に技術的・ビジネス的な実現可能性を検証するステップで、リスクを早期に発見できます。アジャイル開発ではスプリント（1-4週間）を繰り返し、変化に柔軟に対応します。",
    relatedQuestionIds: ["biz-002", "biz-010", "biz-012", "biz-014"],
  },
  {
    id: "biz-topic-02",
    category: "ビジネス力",
    title: "KPI・指標設計と分析手法",
    points: [
      "KPIは最終目標（KGI）達成に向けた中間指標",
      "A/Bテストは2群をランダム割当して効果を統計的に検証",
      "NPS（Net Promoter Score）は顧客ロイヤルティを測る推薦度指標",
      "ダッシュボードは閲覧者のニーズに合わせた情報設計が重要",
    ],
    detail:
      "KPI（重要業績評価指標）は目標達成の進捗を数値で把握するための指標です。例えば売上がKGIなら、月間アクティブユーザー数やコンバージョン率がKPIになります。A/Bテストでは統制群と処理群にランダムに割り当て、統計的に有意な差があるかを検証します。NPSは顧客の推薦意向を0-10で聞き、推奨者(9-10)の割合から批判者(0-6)の割合を引いた値です。",
    relatedQuestionIds: ["biz-001", "biz-003", "biz-005", "biz-017", "biz-020"],
  },
  {
    id: "biz-topic-03",
    category: "ビジネス力",
    title: "データ倫理と法規制",
    points: [
      "個人情報保護法の「要配慮個人情報」: 人種、病歴、犯罪歴など（メールアドレスは非該当）",
      "GDPR（一般データ保護規則）はEUの個人データ保護に関する規制",
      "匿名化は復元不可能、仮名化は追加情報で復元可能",
      "AIの公平性: 学習データの偏りによる特定属性への不利な判断に注意",
    ],
    detail:
      "データ活用にあたっては倫理的・法的な配慮が不可欠です。日本の個人情報保護法は要配慮個人情報に特に厳格な取り扱いを求めます。GDPRはEU域内の個人データ処理・移転に厳しいルールを課し、違反には高額な制裁金があります。AIの公平性は学習データのバイアスにより特定の属性に不公平な判断が生じる問題で、データの偏りの検出と修正が重要です。",
    relatedQuestionIds: ["biz-004", "biz-006", "biz-009", "biz-015"],
  },
  {
    id: "biz-topic-04",
    category: "ビジネス力",
    title: "顧客分析とマーケティング",
    points: [
      "RFM分析: Recency（最終購買日）、Frequency（頻度）、Monetary（金額）で顧客をセグメント化",
      "コホート分析: 同時期のグループごとに行動を追跡・比較",
      "LTV（顧客生涯価値）はLTV > CAC（獲得コスト）が持続性の条件",
      "バスケット分析の支持度（Support）は同時購入の割合",
    ],
    detail:
      "顧客分析はマーケティング施策の最適化に不可欠です。RFM分析は3つの軸で顧客をランク付けし、セグメントごとに適切な施策を打ちます。コホート分析は登録時期別にリテンション率（継続率）を可視化し、改善施策の効果測定に使います。LTVは顧客が生涯にわたってもたらす利益の予測値で、マーケティング投資の判断基準になります。",
    relatedQuestionIds: ["biz-007", "biz-011", "biz-013", "biz-016"],
  },
  {
    id: "biz-topic-05",
    category: "ビジネス力",
    title: "データガバナンスと組織",
    points: [
      "データガバナンスはデータの品質・セキュリティ・利活用の組織的管理体制",
      "DS検定の3つの力: データサイエンス力・データエンジニアリング力・ビジネス力",
      "データドリブン経営は客観的データに基づく意思決定",
      "因果推論: 相関関係は必ずしも因果関係を意味しない",
    ],
    detail:
      "データガバナンスは組織全体でデータの品質管理、アクセス権限、セキュリティ、コンプライアンスなどを体系的に管理する体制です。データサイエンティストにはデータサイエンス力・エンジニアリング力・ビジネス力の3つが求められます。データドリブン経営ではBIツールなどを活用し、経験や勘だけでなくデータに基づいた意思決定を行います。因果推論では交絡因子による見かけの相関に注意が必要です。",
    relatedQuestionIds: ["biz-005", "biz-008", "biz-018", "biz-019"],
  },
];
